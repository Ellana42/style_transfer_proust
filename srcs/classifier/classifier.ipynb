{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ea24595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0118b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "import re\n",
    "from pathlib import Path\n",
    "from torchtext.vocab import FastText, vocab\n",
    "import json\n",
    "import string\n",
    "\n",
    "from srcs.dataset import TextDataset\n",
    "from srcs.embedding import get_fasttext_vectors\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "aae7bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'datasets/dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2276c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    return pd.read_csv(DATASET_PATH, sep='|', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "02e6c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71dc25f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa74b9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_stopwords = stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4789a35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    return [token for token in text if token.lower() not in french_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b68fe018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text_data):\n",
    "    return sent_tokenize(text_data, language='french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43ecf472",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4c6be422",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'''\\w'|\\w+|[^\\w\\s]''')\n",
    "def preprocess(text):\n",
    "    return text\n",
    "\n",
    "def tokenize_sentence(sentence: str,tokenizer: RegexpTokenizer):\n",
    "    '''Simple tokenizer, removes or replaces special characters\n",
    "    sentence : str sentence to be tokenized\n",
    "    tokenizer : tokenizer with tokenize method '''\n",
    "\n",
    "    #Lower capital leters\n",
    "    tokenized=sentence.lower()\n",
    "    #Change special character\n",
    "    tokenized=re.sub(\"’\",\"'\",tokenized)\n",
    "    #Remove unwanted characters\n",
    "    tokenized=re.sub(\"(@\\w*\\\\b\\s?|#\\w*\\\\b\\s?|&\\w*\\\\b\\s?|\\n\\s?|\\\\\\\\|\\<|\\>|\\||\\*)\",\"\",tokenized)\n",
    "    tokenized=re.sub(\"\\/\",\"\",tokenized)\n",
    "    #Replace articles since model does not embed contractions well\n",
    "    tokenized=re.sub(\"l'\",\"le \",tokenized)\n",
    "    tokenized=re.sub(\"d'\",\"de \",tokenized)\n",
    "    tokenized=re.sub(\"j'\",\"je \",tokenized)\n",
    "    tokenized=re.sub(\"qu'\",\"que \",tokenized)\n",
    "    tokenized=re.sub(\"t'\",\"te \",tokenized)\n",
    "    tokenized=re.sub(\"c'\",\"ce \",tokenized)\n",
    "    tokenized = tokenized.lower() \n",
    "    tokenized=tokenized.strip()  \n",
    "    tokenized=re.compile('<.*?>').sub('', tokenized) \n",
    "    tokenized = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', tokenized)  \n",
    "    tokenized = re.sub('\\s+', ' ', tokenized)  \n",
    "    tokenized = re.sub(r'\\[[0-9]*\\]',' ',tokenized) \n",
    "    tokenized=re.sub(r'[^\\w\\s]', '', str(tokenized).lower().strip())\n",
    "    tokenized = re.sub(r'\\d',' ',tokenized) \n",
    "    tokenized = re.sub(r'\\s+',' ',tokenized) \n",
    "      #Tokenize sentence\n",
    "    tokenized=tokenizer.tokenize(tokenized)\n",
    "    return(tokenized)\n",
    "tokenize = lambda text : tokenize_sentence(text, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "12af5e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "v\n",
    "def pad(token_sequence, sequence_size):\n",
    "    '''Function to pad sequence\n",
    "    token_sequence : list of tokens of len < self.sequence_size'''\n",
    "    pad_token=\"<pad>\"\n",
    "    start_token=\"<s>\"\n",
    "    end_token=\"</s>\"\n",
    "    pad_size = sequence_size-len(token_sequence)\n",
    "    result = [start_token] + token_sequence + [pad_token for i in range(pad_size)] + [end_token]\n",
    "    return result\n",
    "\n",
    "def sample(tokens):\n",
    "    ''' sample sequence from token sequence\n",
    "    tokens : list of tokens'''\n",
    "    pad_token=\"<pad>\",\n",
    "    start_token=\"<s>\",\n",
    "    end_token=\"</s>\"\n",
    "\n",
    "    nb_tokens=len(tokens)\n",
    "    starting_index=np.random.randint(nb_tokens)\n",
    "    if starting_index + self.sequence_size < nb_tokens :\n",
    "      result = [self.start_token] + tokens[starting_index : starting_index + self.sequence_size] + [self.end_token]\n",
    "    else:\n",
    "      result= tokens[starting_index : nb_tokens]\n",
    "      result=self.pad(result)\n",
    "    return result\n",
    "\n",
    "def process(tokens):\n",
    "    sequence=[vocab_stoi[token] for token in sample(tokens)]\n",
    "    return (sequence,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "22bba023",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df.text.apply(str).apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3bd106ce",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 0/356898 [00:00<?, ?it/s]Skipping token b'1152449' with 1-dimensional vector [b'300']; likely a header\n",
      " 12%|████████▋                                                               | 43235/356898 [00:02<00:20, 15538.14it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [116]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ft_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mget_fasttext_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m embedder \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding\u001b[38;5;241m.\u001b[39mfrom_pretrained(ft_vectors\u001b[38;5;241m.\u001b[39mvectors)\n",
      "File \u001b[0;32m~/projects/style_transfer_proust/srcs/embedding.py:5\u001b[0m, in \u001b[0;36mget_fasttext_vectors\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_fasttext_vectors\u001b[39m():\n\u001b[0;32m----> 5\u001b[0m     ft_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mFastText\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     vocab_stoi \u001b[38;5;241m=\u001b[39m ft_vectors\u001b[38;5;241m.\u001b[39mstoi\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#vocab takes a counter as argument, it keeps the order of the dictionnary and sets the indexes according to these indexes.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#vocab ignores all the words with 0 occurences, since here what we have is an stoi dictionnary and not a counter\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#We have to give value at least 1 to the element which was assigned index 0 other wise it won't be included in the vocab object\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/style_transfer_proust/.venv/lib/python3.9/site-packages/torchtext/vocab/vectors.py:230\u001b[0m, in \u001b[0;36mFastText.__init__\u001b[0;34m(self, language, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl_base\u001b[38;5;241m.\u001b[39mformat(language)\n\u001b[1;32m    229\u001b[0m name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[0;32m--> 230\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFastText\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/style_transfer_proust/.venv/lib/python3.9/site-packages/torchtext/vocab/vectors.py:59\u001b[0m, in \u001b[0;36mVectors.__init__\u001b[0;34m(self, name, cache, url, unk_init, max_vectors)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munk_init \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor\u001b[38;5;241m.\u001b[39mzero_ \u001b[38;5;28;01mif\u001b[39;00m unk_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m unk_init\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_vectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_vectors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/style_transfer_proust/.venv/lib/python3.9/site-packages/torchtext/vocab/vectors.py:151\u001b[0m, in \u001b[0;36mVectors.cache\u001b[0;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[1;32m    148\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping non-UTF8 token \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mrepr\u001b[39m(word)))\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m vectors[vectors_loaded] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mfloat\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries])\n\u001b[1;32m    152\u001b[0m vectors_loaded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    153\u001b[0m itos\u001b[38;5;241m.\u001b[39mappend(word)\n",
      "File \u001b[0;32m~/projects/style_transfer_proust/.venv/lib/python3.9/site-packages/torchtext/vocab/vectors.py:151\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    148\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping non-UTF8 token \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mrepr\u001b[39m(word)))\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m vectors[vectors_loaded] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries])\n\u001b[1;32m    152\u001b[0m vectors_loaded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    153\u001b[0m itos\u001b[38;5;241m.\u001b[39mappend(word)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ft_vectors, vocab_stoi = get_fasttext_vectors()\n",
    "embedder = nn.Embedding.from_pretrained(ft_vectors.vectors, freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d69c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence=[self.stoi[token] for token in self.sample(tokens)]\n",
    "    return(sequence,label)\n",
    "\n",
    "embedded = embedder.embed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bb41196d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Les \"grands dangers\" qui guettent les voyageur...</td>\n",
       "      <td>0</td>\n",
       "      <td>les grands dangers qui guettent les voyageurs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nOn prête au premier ministre de l'État d'I...</td>\n",
       "      <td>0</td>\n",
       "      <td>on prête au premier ministre de le état de isr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L e cortège prend des allures de visite guidée...</td>\n",
       "      <td>0</td>\n",
       "      <td>l e cortège prend des allures de visite guidée...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U ne pluie de graines et d’informations s’est ...</td>\n",
       "      <td>0</td>\n",
       "      <td>u ne pluie de graines et de informations s est...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fiscalité du patrimoine : ce que proposent les...</td>\n",
       "      <td>0</td>\n",
       "      <td>fiscalité du patrimoine ce que proposent les c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>Je pourrais, bien que l'erreur soit plus grave...</td>\n",
       "      <td>1</td>\n",
       "      <td>je pourrais bien que le erreur soit plus grave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>Je me disais aussi : \" Non seulement est-il en...</td>\n",
       "      <td>1</td>\n",
       "      <td>je me disais aussi non seulement est il encore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>J'éprouvais un sentiment de fatigue profonde à...</td>\n",
       "      <td>1</td>\n",
       "      <td>je éprouvais un sentiment de fatigue profonde ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>La date à laquelle j'entendais le bruit de la ...</td>\n",
       "      <td>1</td>\n",
       "      <td>la date à laquelle je entendais le bruit de la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Je venais de comprendre pourquoi le duc de Gue...</td>\n",
       "      <td>1</td>\n",
       "      <td>je venais de comprendre pourquoi le duc de gue...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21413 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  \\\n",
       "0    Les \"grands dangers\" qui guettent les voyageur...      0   \n",
       "1      \\nOn prête au premier ministre de l'État d'I...      0   \n",
       "2    L e cortège prend des allures de visite guidée...      0   \n",
       "3    U ne pluie de graines et d’informations s’est ...      0   \n",
       "4    Fiscalité du patrimoine : ce que proposent les...      0   \n",
       "..                                                 ...    ...   \n",
       "243  Je pourrais, bien que l'erreur soit plus grave...      1   \n",
       "244  Je me disais aussi : \" Non seulement est-il en...      1   \n",
       "245  J'éprouvais un sentiment de fatigue profonde à...      1   \n",
       "246  La date à laquelle j'entendais le bruit de la ...      1   \n",
       "247  Je venais de comprendre pourquoi le duc de Gue...      1   \n",
       "\n",
       "                                             tokenized  \n",
       "0    les grands dangers qui guettent les voyageurs ...  \n",
       "1    on prête au premier ministre de le état de isr...  \n",
       "2    l e cortège prend des allures de visite guidée...  \n",
       "3    u ne pluie de graines et de informations s est...  \n",
       "4    fiscalité du patrimoine ce que proposent les c...  \n",
       "..                                                 ...  \n",
       "243  je pourrais bien que le erreur soit plus grave...  \n",
       "244  je me disais aussi non seulement est il encore...  \n",
       "245  je éprouvais un sentiment de fatigue profonde ...  \n",
       "246  la date à laquelle je entendais le bruit de la...  \n",
       "247  je venais de comprendre pourquoi le duc de gue...  \n",
       "\n",
       "[21413 rows x 3 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2b31fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ea9090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_word_embedding(sentence):\n",
    "    # Tokeniser la phrase\n",
    "    doc = nlp(sentence)\n",
    "    # Retourner le vecteur lié à chaque token\n",
    "    return [(X.vector) for X in doc]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "style_transfer",
   "language": "python",
   "name": "style_transfer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
