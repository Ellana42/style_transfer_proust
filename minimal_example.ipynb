{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpOWS7rYiAXP"
   },
   "source": [
    "#Run to import modules + define fcts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "V-5bRKxihiRe"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import functools\n",
    "import torch\n",
    "import gdown\n",
    "\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "\n",
    "from math import log\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer,TweetTokenizer\n",
    "\n",
    "\n",
    "from torchtext.vocab import FastText,vocab\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2_6F44FNhpDe"
   },
   "outputs": [],
   "source": [
    "#Simple regular expression based tokenizer which tokenizes based on non alpha numeric characters, and which\n",
    "#tokenizes \"d'une\" as \"d'\" and \"une\"\n",
    "tokenizer = RegexpTokenizer(r'''\\w'|\\w+|[^\\w\\s]''')\n",
    "\n",
    "\n",
    "def tokenize_sentence(sentence: str,tokenizer: RegexpTokenizer):\n",
    "  '''Simple tokenizer, removes or replaces special characters\n",
    "  sentence : str sentence to be tokenized\n",
    "  tokenizer : tokenizer with tokenize method '''\n",
    "\n",
    "  #Lower capital leters\n",
    "  tokenized=sentence.lower()\n",
    "  #Change special character\n",
    "  tokenized=re.sub(\"’\",\"'\",tokenized)\n",
    "  #Remove unwanted characters\n",
    "  tokenized=re.sub(\"(@\\w*\\\\b\\s?|#\\w*\\\\b\\s?|&\\w*\\\\b\\s?|\\n\\s?|\\\\\\\\|\\<|\\>|\\||\\*)\",\"\",tokenized)\n",
    "  tokenized=re.sub(\"\\/\",\"\",tokenized)\n",
    "  #Replace articles since model does not embed contractions well\n",
    "  tokenized=re.sub(\"l'\",\"le \",tokenized)\n",
    "  tokenized=re.sub(\"d'\",\"de \",tokenized)\n",
    "  tokenized=re.sub(\"j'\",\"je \",tokenized)\n",
    "  tokenized=re.sub(\"qu'\",\"que \",tokenized)\n",
    "  tokenized=re.sub(\"t'\",\"te \",tokenized)\n",
    "  tokenized=re.sub(\"c'\",\"ce \",tokenized)\n",
    "  #Tokenize sentence\n",
    "  tokenized=tokenizer.tokenize(tokenized)\n",
    "  return(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WhHsO1L3iQqH"
   },
   "outputs": [],
   "source": [
    "class text_Dataset(Dataset):\n",
    "  '''Class to be used to generate a pytorch dataloader of sentences extracted from french articles '''\n",
    "\n",
    "  def __init__(self, paths, tokenizer_function, vocab_stoi, sequence_size=10, df =None, pad_token=\"<pad>\", start_token=\"<s>\", end_token=\"</s>\",file_type=\"csv\"):\n",
    "    '''Instantiate french article dataset class\n",
    "    path_to_articles : list containing path\n",
    "    tokenizer_function : callable function which tokenizes a string \n",
    "    vocab_stoi : stoi indexing\n",
    "    sequence_size : len of sentence sampled without start of sentence and end of sentence token\n",
    "    pad_token : padding token default <pad>\n",
    "    start_token : start of sentence token default <s>\n",
    "    end_token : end of sentence token default </s>'''\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self.paths = paths#list containing list of paths to articles and proust\n",
    "    self.tokenizer_function = tokenizer_function#tokenizer used\n",
    "    self.sequence_size = sequence_size\n",
    "    self.pad_token = pad_token\n",
    "    self.start_token = start_token\n",
    "    self.end_token = end_token\n",
    "    self.stoi=vocab_stoi\n",
    "    self.file_type=file_type\n",
    "    self.df=df\n",
    "\n",
    "    if self.file_type == \"json\":\n",
    "      self.len_data = len(self.paths)\n",
    "    else:\n",
    "      self.len_data = len(df)\n",
    "      pass\n",
    "  def __len__(self):\n",
    "    '''number of elements in the dataset'''\n",
    "\n",
    "    return(self.len_data)\n",
    "  \n",
    "\n",
    "  def pad(self,token_sequence):\n",
    "    '''Function to pad sequence\n",
    "    token_sequence : list of tokens of len < self.sequence_size'''\n",
    "\n",
    "    pad_size = self.sequence_size-len(token_sequence)\n",
    "    result = [self.start_token] + token_sequence + [self.pad_token for i in range(pad_size)] + [self.end_token]\n",
    "    \n",
    "    return result\n",
    "  \n",
    "\n",
    "  def sample(self,tokens):\n",
    "    ''' sample sequence from token sequence\n",
    "    tokens : list of tokens'''\n",
    "\n",
    "    nb_tokens=len(tokens)\n",
    "    starting_index=np.random.randint(nb_tokens)\n",
    "\n",
    "    \n",
    "    if starting_index + self.sequence_size < nb_tokens :\n",
    "      result = [self.start_token] + tokens[starting_index : starting_index + self.sequence_size] + [self.end_token]\n",
    "    \n",
    "    else:\n",
    "      result= tokens[starting_index : nb_tokens]\n",
    "      result=self.pad(result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "    pass\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    '''get a sentence of size sequence_size from file of index idx\n",
    "    '''\n",
    "\n",
    "    if self.file_type==\"json\":\n",
    "      text_file=read_json(self.paths[idx])\n",
    "\n",
    "      label=text_file[\"label\"]\n",
    "      text=text_file[\"text\"]\n",
    "\n",
    "      tokens=self.tokenizer_function(text)\n",
    "      \n",
    "      sequence=[self.stoi[token] for token in self.sample(tokens)]\n",
    "    \n",
    "    else:\n",
    "\n",
    "      label=(self.df).iloc[idx][\"label\"]\n",
    "      text=(self.df).iloc[idx][\"text\"]\n",
    "\n",
    "      tokens=self.tokenizer_function(text)\n",
    "      sequence=[self.stoi[token] for token in self.sample(tokens)]\n",
    "\n",
    "    return(torch.LongTensor(sequence),label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sx8C2e9ihzea"
   },
   "outputs": [],
   "source": [
    "class rnn(nn.Module):\n",
    "  '''simple single gru cell rnn class'''\n",
    "\n",
    "  def __init__(self, param_dicts,  pretrained_embedding = None ):\n",
    "    '''Instantiates rnn encoder class\n",
    "    param_dicts : dictionnary of parameter\n",
    "    pretrained_embedding : vectors embedding with __getitem__ method\n",
    "    batch_size : batch size\n",
    "    '''\n",
    "    \n",
    "    super().__init__()\n",
    "\n",
    "    try:\n",
    "      self.nb_layers = param_dicts[\"nb_layers\"]\n",
    "    except:\n",
    "      self.nb_layers = 1\n",
    "    \n",
    "    try:\n",
    "      self.embed_size = param_dicts[\"embed_size\"]\n",
    "    except:\n",
    "      self.embed_size = 300\n",
    "    \n",
    "    try:\n",
    "      self.hidden_dim = param_dicts[\"hidden_dim\"]\n",
    "    except:\n",
    "      self.hidden_dim = 256\n",
    "    \n",
    "    #instantiates gru function with imput of dimension embed_size and hidden state of the same dimension\n",
    "    self.gru=nn.GRU(self.embed_size, self.hidden_dim, num_layers = self.nb_layers, batch_first = True)\n",
    "  \n",
    "  \n",
    "  def forward(self, x, initial_state ):\n",
    "\n",
    "    '''forward function\n",
    "    x : batch of sequences\n",
    "    state : previous hidden state'''\n",
    "    \n",
    "    previous_states,final_state = self.gru(x,initial_state)\n",
    "\n",
    "    return previous_states, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JiSpNrAFh8SZ"
   },
   "outputs": [],
   "source": [
    "class style_transfer(nn.Module):\n",
    "  '''Style transfer class.  '''\n",
    "  def __init__(self, parameters_dicts, pretrained_embedding):\n",
    "    '''parameters_dict : dictionnary containing parameters \n",
    "    batch_size : batch size'''\n",
    "    super().__init__()\n",
    "\n",
    "    #First of all retrieve general parameters from parameter dictionnary\n",
    "    #label smoothing parameter\n",
    "    try:\n",
    "      self.label_smoothing_sigma = parameters_dicts[\"style\"][\"label_smoothing_sigma\"]\n",
    "    except:\n",
    "      self.label_smoothing_sigma = 0.1\n",
    "    #learning rate\n",
    "    try :\n",
    "      self.lr = parameters_dicts[\"style\"][\"lr\"]\n",
    "    except:\n",
    "      self.lr = 0.001\n",
    "    # Style embedding dimension\n",
    "    try :\n",
    "      self.style_embed_dim = parameters_dicts[\"style\"][\"style_embed_dim\"]\n",
    "    except :\n",
    "      self.style_embed_dim = 128\n",
    "    #Max sequence length when generating sequence with forward\n",
    "    try:\n",
    "      self.max_generated_sequence_len = parameters_dicts[\"style\"][\"max_generated_sequence_len\"]\n",
    "    except:\n",
    "      self.max_generated_sequence_len = 15\n",
    "    #end of sentence index\n",
    "    try:\n",
    "      self.eos_idx = parameters_dicts[\"style\"][\"eos_idx\"]\n",
    "    except:\n",
    "      self.eos_idx = 1\n",
    "    #start of sentence index\n",
    "    try:\n",
    "      self.sos_idx = parameters_dicts[\"style\"][\"sos_idx\"]\n",
    "    except:\n",
    "      self.sos_idx = 2#1152451\n",
    "    #size of the vocabulary\n",
    "    try:\n",
    "      self.vocab_size = parameters_dicts[\"style\"][\"vocab_size\"]\n",
    "    except:\n",
    "      self.vocab_size = pretrained_embedding.shape[0]\n",
    "    # word embedding size\n",
    "    try :\n",
    "      self.embed_size = parameters_dicts[\"style\"][\"embed_size\"]\n",
    "    except :\n",
    "      self.embed_size = 10\n",
    "    # number of additionnal tokens\n",
    "    try :\n",
    "      self.additionnal_tokens = parameters_dicts[\"style\"][\"additionnal_tokens\"]\n",
    "    except :\n",
    "      self.additionnal_tokens = 3 #pad, sos, unk\n",
    "    #dropout probability\n",
    "    try :\n",
    "      self.dropout_p = parameters_dicts[\"style\"][\"dropout_p\"]\n",
    "    except:\n",
    "      self.dropout_p = 0.3\n",
    "    #batch size\n",
    "    try :\n",
    "      self.batch_size = parameters_dicts[\"style\"][\"batch_size\"]\n",
    "    except:\n",
    "      self.batch_size = 1\n",
    "\n",
    "\n",
    "    #Parameter of gumbel softmax\n",
    "    self.tau = 1.0\n",
    "    # For eval to eval on batches of size != batch size\n",
    "    self.size = self.batch_size\n",
    "    ##########################################################################################################\n",
    "    # Autoencoder\n",
    "    ##########################################################################################################\n",
    "    \n",
    "    #Define encoder\n",
    "    try:\n",
    "      encoder_dict = parameters_dicts[\"autoencoder\"][\"encoder\"]\n",
    "    except:\n",
    "      encoder_dict = None\n",
    "    encoder = rnn(encoder_dict)\n",
    "\n",
    "    self.hidden_dim = encoder.hidden_dim\n",
    "\n",
    "    #Define decoder\n",
    "    try:\n",
    "      decoder_dict = parameters_dicts[\"autoencoder\"][\"decoder\"]\n",
    "    except:\n",
    "      decoder_dict = None\n",
    "    \n",
    "    decoder = rnn(decoder_dict)\n",
    "    \n",
    "    #Define linear network used to get a vector of the size of the vocabulary from result of decoder\n",
    "    self.convert_to_vocab = nn.Linear(self.hidden_dim, self.vocab_size + self.additionnal_tokens)\n",
    "\n",
    "\n",
    "    #Autoencoder\n",
    "    self.autoencoder={\"encoder\" : encoder, \"decoder\" : decoder}\n",
    "\n",
    "    self.dropout_layer=nn.Dropout(self.dropout_p)\n",
    "\n",
    "    self.encoder_initial_state = None\n",
    "    self.decoder_initial_state = None\n",
    "    ##########################################################################################################\n",
    "    # Layers used to embed text idx tokens\n",
    "    ##########################################################################################################\n",
    "    \n",
    "\n",
    "    #Below : in pretrained_embed we create an Embedding layer from the pretrained embedding vectors given as argument,\n",
    "    #We also add an embedding vector full of 0s which will be associated to padding token\n",
    "    embedding_vectors=torch.cat( (pretrained_embedding, torch.zeros(size=(1,self.embed_size))))\n",
    "    self.pretrained_embed=nn.Embedding.from_pretrained(embedding_vectors, freeze=True, padding_idx = self.vocab_size)\n",
    "\n",
    "    #Below : in learnable_embedding we create an Embedding layer consisting of 2 trainable vectors.\n",
    "    #First vector will be embedding of <unk> token and second will be embedding of <s> token\n",
    "    self.learnable_embed=nn.Embedding.from_pretrained( torch.randn(size=(2,self.embed_size)), freeze=False )\n",
    "\n",
    "    ##########################################################################################################\n",
    "    # Layers used to embed style\n",
    "    ##########################################################################################################\n",
    "\n",
    "    #Simple linear model to embed style (0 or 1 labels) into vector of defined style size\n",
    "    self.encoder_label_embedder = nn.Linear(1, self.style_embed_dim)\n",
    "    self.decoder_label_embedder = nn.Linear(1, self.style_embed_dim)\n",
    "\n",
    "    ################################################\n",
    "    #Loss / optimizer\n",
    "    #################################\n",
    "\n",
    "    self.reconstruction_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    self.autoencoder_optim = torch.optim.Adam([{\"params\" : self.learnable_embed.parameters()},\n",
    "            {'params': (self.autoencoder)[\"encoder\"].parameters()},\n",
    "            {'params': (self.autoencoder)[\"decoder\"].parameters()},\n",
    "             {'params': self.encoder_label_embedder.parameters()},\n",
    "             {'params': self.decoder_label_embedder.parameters()},\n",
    "             {'params': self.convert_to_vocab.parameters()}],\n",
    "            lr=self.lr)\n",
    "\n",
    "    return None #end of __init__\n",
    "  \n",
    "  #######################################################################################################################################\n",
    "\n",
    "  #############################################\n",
    "  #\n",
    "  #############################################\n",
    "  def get_final_hiddens(self,lengths,hiddens):\n",
    "    '''We can hope to predict an eos token. In this case, we want to retrieve the hidden state right after the eos was predicted (and not the later ones)\n",
    "    lengths : int tensor which contains the position in which the eos token has been predicted in our generated sentences\n",
    "    hiddens : list of final hidden layer. Shape (batch_size,max_seq_length,hidden_dim), hiddens[i,j] is the hidden state obtained\n",
    "    after predicting the jth token of the ith batch using the previous j-1 tokens'''\n",
    "\n",
    "    final_hiddens = torch.zeros((self.size,1, self.hidden_size))\n",
    "    for i in range(self.size):\n",
    "      final_hiddens[i,:,:] = hiddens[i,lengths[i],:]\n",
    "    return final_hiddens\n",
    "\n",
    "  \n",
    "  #############################################\n",
    "  #embbeding from token indexes function\n",
    "  #############################################\n",
    "  def embed(self,x):\n",
    "    '''Embedding of input using the 2 embedding layers defined above\n",
    "    x : batch input or text indexes of dim 2 (batch_size,sequence_size) ex [[0,1,2],[3,4,5]] '''\n",
    "    \n",
    "    embedding_mask = x> self.vocab_size #mask : tensor of boolean, True means that index belongs to either <unk> or to <s> (not in pretrained vectors)\n",
    "\n",
    "    pretrained_x = x.clone() #In this vector, we will store embedding of the tokens which are not <unk> or <s>\n",
    "    \n",
    "    #Set indexes of <unk> and <s> to the index of padding index, embedding of these vectors is zero tensor\n",
    "    pretrained_x[embedding_mask] = self.vocab_size\n",
    "\n",
    "    #Now embed this vector using pretrained embedding vectors\n",
    "    embedded_x = self.pretrained_embed(pretrained_x)\n",
    "    embedded_x[embedding_mask] = self.learnable_embed(x[embedding_mask]-self.vocab_size-1)\n",
    "\n",
    "    return embedded_x\n",
    "\n",
    "  #############################################\n",
    "  # Label smoothing function\n",
    "  #############################################\n",
    "  def label_smoothing(self,labels) :\n",
    "    ''' return a smooth (noisy) version of the labels \n",
    "    labels : batch tensor of labels (0 or 1) dim 3 (batch_size,1,1)'''\n",
    "    \n",
    "    mask = (labels == 1)#Label 1\n",
    "    labels=torch.tensor(labels).float()\n",
    "\n",
    "    noise=torch.rand(mask.sum())\n",
    "    labels[mask] -= self.label_smoothing_sigma * noise#remove noise to labels 1\n",
    "    \n",
    "    noise=torch.rand((~mask).sum())\n",
    "    labels[~mask] += self.label_smoothing_sigma *noise#add noise to labels 0\n",
    "\n",
    "    return labels\n",
    "  \n",
    "  #############################################\n",
    "  # Use Gumbel trick to get embedding\n",
    "  #############################################\n",
    "\n",
    "  def gumbel_word_sample(self, embeddings, output):\n",
    "    '''Uses gumbel softmax to differentiably sample hot encoded vector and return average embedding using softmax.\n",
    "    embeddings : embedding vectors\n",
    "    output : last hidden state of the decoder (embeddings of size hidden_dim)  '''\n",
    "\n",
    "    if self.training:\n",
    "      drop = nn.Dropout(p=0.2)\n",
    "      output=drop(output)\n",
    "      \n",
    "    vocab_embedd = self.convert_to_vocab(output)#embed to vocabulary size\n",
    "    gumbel_softmax_output=F.gumbel_softmax(vocab_embedd, tau = self.tau)#apply gumber softmax\n",
    "\n",
    "    token_approx = torch.matmul(gumbel_softmax_output,embeddings)#average embeddings by softmax probability to get new embedding\n",
    "\n",
    "    return token_approx, gumbel_softmax_output\n",
    "  \n",
    "  #############################################\n",
    "  # Create initial states from both the encoder and the generator\n",
    "  #############################################\n",
    "  def initial_state_embedder(self, input, labels, same_labels = True, obj_labels = None):\n",
    "    '''Create initial hidden state for rnn training. Encoder's state is stored under  self.encoder_initial_state and decoder's state under \n",
    "    self.decoder_initial_state. returns None\n",
    "    input : inputs of the encoder (batch_size,seq_len, embed_dim)\n",
    "    labels : style labels 3 (batch_size,1,1)\n",
    "    same_labels : use same labels / styles for encoder and decoder\n",
    "    obj_labels : if same_labels is set to False, use obj_labels embedding as input to the decoder\n",
    "    '''\n",
    "    \n",
    "    #First smooth the labels\n",
    "    smooth_labels=self.label_smoothing(labels.float())\n",
    "    #Get encoder style embedding\n",
    "    encoder_style_embed = self.encoder_label_embedder(smooth_labels)\n",
    "    #Create encoder's initial state\n",
    "    self.encoder_initial_state = torch.cat((encoder_style_embed , torch.zeros(size=(self.size, 1, self.hidden_dim - self.style_embed_dim)) ), 2)\n",
    "    self.encoder_initial_state = self.encoder_initial_state.view(1,self.size,self.hidden_dim)\n",
    "\n",
    "    ###################################################\n",
    "    #Now we want the decoder's initial state. To do that, since the decoder is content dependent, we need to retrieve the content\n",
    "    #embedding from the output of the encoder\n",
    "    \n",
    "    # First if we want the outputs to have the same style as the input\n",
    "    if same_labels:\n",
    "      \n",
    "      _, encoder_final_state = self.autoencoder[\"encoder\"](input,self.encoder_initial_state)\n",
    "      content = encoder_final_state[:,:,self.style_embed_dim:] #retrive from hidden states embeddings of sizes content_dim representing the content\n",
    "\n",
    "      #Now, using this content and the encoder's inputs, we will create the decoder's input\n",
    "      decoder_style_embed = self.decoder_label_embedder(smooth_labels)\n",
    "      self.decoder_initial_state = torch.cat( (decoder_style_embed.view(1,self.size,-1), content.view(1,self.size,-1)) , 2)\n",
    "    \n",
    "    # Now if we want the outputs to have the styles encoded in other_labels\n",
    "    else:\n",
    "\n",
    "      smooth_obj_labels = self.label_smoothing(obj_labels.float())\n",
    "      _, encoder_final_state = self.autoencoder[\"encoder\"](input,self.encoder_initial_state)\n",
    "      content = encoder_final_state[:,:,self.style_embed_dim:] #retrive from hidden states embeddings of sizes content_dim representing the content\n",
    "\n",
    "      decoder_style_embed = self.decoder_label_embedder(smooth_obj_labels)\n",
    "      self.decoder_initial_state = torch.cat( (decoder_style_embed.view(1,self.size,-1), content.view(1,self.size,-1)) , 2)\n",
    "    \n",
    "    return None\n",
    "\n",
    "  #############################################\n",
    "  # get encoder's outputs\n",
    "  #############################################\n",
    "  def run_encoder(self, x):\n",
    "    ''' get output and last hidden state of embedding x, creating the initial states using the smooth labels and passing them to encoder\n",
    "    x : embedded_input\n",
    "    initial_state : self.encoder_initial_state'''\n",
    "\n",
    "    \n",
    "    state = self.encoder_initial_state\n",
    "    output,result=self.autoencoder[\"encoder\"](x,state)\n",
    "\n",
    "    return(output,result)\n",
    "  \n",
    "  #############################################\n",
    "  # autoencode data\n",
    "  #############################################\n",
    "  def generate_tokens(self, embedded_tokens):\n",
    "      '''given a list of embedded_tokens, return all the outputs of the decoder embedded into vocab size using convert_to_vocab and raw\n",
    "      embedded_tokens : current tokens generated'''\n",
    "\n",
    "      initial_state = self.decoder_initial_state\n",
    "      output, hidden_state = self.autoencoder[\"decoder\"](embedded_tokens, initial_state)\n",
    "\n",
    "\n",
    "      if self.training:\n",
    "        output = self.dropout_layer(output)\n",
    "\n",
    "      to_vocab_ = self.convert_to_vocab(output)\n",
    "      return to_vocab_, output\n",
    "\n",
    "  #############################################\n",
    "  # get batch reconstruction loss\n",
    "  #############################################\n",
    "  def batch_loss(self, gen_input, targets_tokens, labels, eval_size=None  ):\n",
    "    '''compute loss over a batch\n",
    "    gen_input : output of encoder\n",
    "    targets_tokens : indexes\n",
    "    labels : style labels\n",
    "    '''\n",
    "\n",
    "    if self.training:\n",
    "      self.size = self.batch_size\n",
    "    else:\n",
    "      self.size = eval_size\n",
    "    \n",
    "    reconstruction_loss =0\n",
    "    f_labels = labels\n",
    "\n",
    "    self.initial_state_embedder(gen_input, labels, True)\n",
    "    vocab_output, _ = self.generate_tokens(gen_input)\n",
    "\n",
    "    loss = self.reconstruction_loss(vocab_output.view(-1,self.vocab_size + self.additionnal_tokens),targets_tokens.view(-1))\n",
    "\n",
    "    return(loss)\n",
    "  \n",
    "  #############################################\n",
    "  # train model on batch\n",
    "  #############################################\n",
    "  def train_batch(self, input_idx_tokens, labels, cur_iter, iter_verbose = 100):\n",
    "    '''train model en batch\n",
    "    input_idx_tokens : input of token vocab indexes\n",
    "    labels : style labels\n",
    "    cur_iter : current_iteration'''\n",
    "    self.train()#train mode\n",
    "    self.autoencoder_optim.zero_grad()#zero gradient\n",
    "    gen_input=self.embed(input_idx_tokens)#embed input indexes\n",
    "    labels = labels.float()\n",
    "\n",
    "    loss = self.batch_loss(gen_input, torch.tensor(input_idx_tokens), labels)\n",
    "\n",
    "    if cur_iter % iter_verbose == 0 :\n",
    "      print(\"reconstruction loss is\", loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    self.autoencoder_optim.step()\n",
    "    return loss\n",
    "  \n",
    "  #############################################\n",
    "  # eval model on batch\n",
    "  #############################################\n",
    "  def eval_batch(self, input_idx_tokens, labels):\n",
    "    self.eval()\n",
    "    gen_input=self.embed(input_idx_tokens)\n",
    "    labels = labels\n",
    "\n",
    "    loss = self.batch_loss(gen_input, input_idx_tokens, labels, len(input_idx_tokens))\n",
    "    return loss\n",
    "  \n",
    "  #############################################\n",
    "  # generate sequence\n",
    "  #############################################\n",
    "  def hiddens_tokens_embed_generation(self):\n",
    "    '''returns list of last hiddens states and tokens given by model for every step (step : 1 generate token from <bos> , step 2\n",
    "    generate from <bos> and generated output ...\n",
    "    '''\n",
    "\n",
    "    \n",
    "    #get all embeddings : pretrained + trainable ones\n",
    "    embeddings=torch.cat((self.pretrained_embed.parameters().__next__(), self.learnable_embed.parameters().__next__())).clone()\n",
    "    #initial state\n",
    "    state=self.decoder_initial_state\n",
    "    \n",
    "    hiddens = torch.zeros(self.size, self.max_generated_sequence_len, self.hidden_dim)\n",
    "    tokens = torch.zeros(self.size, self.max_generated_sequence_len, self.embed_size)\n",
    "\n",
    "    sentence=(torch.tensor([[self.sos_idx]])).repeat((self.size,1))\n",
    "    #predicted_indexes=torch.tensor([self.sos_idx]).repeat((self.batch_size,1,1))\n",
    "\n",
    "    lengths=torch.tensor([self.max_generated_sequence_len for k in range(self.size)])\n",
    "\n",
    "    for i in range(self.max_generated_sequence_len):\n",
    "\n",
    "      sentence_embedding=self.embed(sentence)\n",
    "      _,state = (self.autoencoder)[\"decoder\"](sentence_embedding,state)\n",
    "      #to_vocab = self.convert_to_vocab(state)\n",
    "      token_approx, gumbel_softmax_output = self.gumbel_word_sample(embeddings , state)\n",
    "      gumbel_softmax_output = gumbel_softmax_output.view(self.size,self.vocab_size+self.additionnal_tokens)\n",
    "      predicted_indexes = torch.argmax(gumbel_softmax_output.squeeze(1),1)\n",
    "      predicted_eos_mask = predicted_indexes == self.eos_idx\n",
    "\n",
    "      lengths[predicted_eos_mask] = i #set index of eos token (current length + 1 for eos token)\n",
    "\n",
    "      sentence = torch.cat((sentence, predicted_indexes.view(self.size,1)),1)\n",
    "\n",
    "      hiddens[:,i,:] = state\n",
    "      tokens[:,i,:] = token_approx\n",
    "\n",
    "    return hiddens , tokens, lengths, sentence\n",
    "  \n",
    "  #############################################\n",
    "  # forward function, to be called only to predict a sequence of opposite style from input\n",
    "  #############################################\n",
    "  def predict(self,input_idx_tokens, input_labels, obj_labels):\n",
    "    '''return predicted index sequence\n",
    "    input_idx_tokens : input index tokens sequence\n",
    "    labels : style labels'''\n",
    "    #eval mode\n",
    "    self.eval()\n",
    "    #get sequences length\n",
    "    self.size = len(input_idx_tokens)\n",
    "    #embed tokens\n",
    "    gen_input=self.embed(input_idx_tokens)\n",
    "    #Now create initial hidden state for the encoder using the real input labels\n",
    "    #And for the decoder using the objective labels\n",
    "    self.initial_state_embedder(gen_input,input_labels, False, obj_labels)\n",
    "    #We generate the predicted sentences. The function takes no argument as\n",
    "    # the output of the encoder was taken into account when\n",
    "    #generating the initial state of the decoder\n",
    "    _,_,_,predicted_sentences = self.hiddens_tokens_embed_generation()\n",
    "\n",
    "    return(predicted_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJOGNCqRh9cP"
   },
   "source": [
    "# Download custom embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Cq9kXu_Hh_HP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=\n",
      "To: /home/ellana/projects/style_transfer_proust/embeddings/embedding.pt\n",
      "1.69kB [00:00, 2.72MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1RshSrc75VhX-5p0cQwDKO_lrIVR7AKbr\n",
      "To: /home/ellana/projects/style_transfer_proust/embedding.pt\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36.9M/36.9M [00:00<00:00, 91.4MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1OBb9ZDGwPuZv3crnBYFNYkpbfLHeZGZp\n",
      "To: /home/ellana/projects/style_transfer_proust/itos.pickle\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 551k/551k [00:00<00:00, 29.5MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1NPqaoWQ7cdEnedc-ZIXA67czLVy0WgpO\n",
      "To: /home/ellana/projects/style_transfer_proust/stoi.pickle\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 643k/643k [00:00<00:00, 27.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "ids = [\"1RshSrc75VhX-5p0cQwDKO_lrIVR7AKbr\", \"1OBb9ZDGwPuZv3crnBYFNYkpbfLHeZGZp\", \"1NPqaoWQ7cdEnedc-ZIXA67czLVy0WgpO\"]\n",
    "filenames = [\"embedding.pt\", \"itos.pickle\", \"stoi.pickle\"]\n",
    "url = \"https://drive.google.com/uc?id=\"\n",
    "output = \"embeddings/embedding.pt\"\n",
    "gdown.download(url, output, quiet=False)\n",
    "for filename, idx in zip(filenames, ids):\n",
    "    gdown.download(url=(url + idx), output=filename, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oHPps4llilgP"
   },
   "outputs": [],
   "source": [
    "pretrained_vectors = torch.load(\"embedding.pt\")\n",
    "\n",
    "with open(\"stoi.pickle\",\"rb\") as f:\n",
    "  stoi = pickle.load(f)\n",
    "\n",
    "with open(\"itos.pickle\",\"rb\") as f:\n",
    "  itos = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_yeTUnUiswU"
   },
   "source": [
    "set vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y82phKIBiqZQ",
    "outputId": "1d7f9767-1b16-4c03-e27d-122188c30a30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index of </s> token is 0\n"
     ]
    }
   ],
   "source": [
    "ft_vocab=vocab(stoi)\n",
    "print(\"index of </s> token is\",ft_vocab[\"</s>\"])\n",
    "#Now add special tokens at the end of the vocab to avoid embedding issues later on\n",
    "#since inserting them in the middle of the stoi vocab will shift all other indexes, which would mess us the embedding\n",
    "#when using nn.embedding later on\n",
    "index_pad=len(ft_vocab)#padding\n",
    "index_unk=len(ft_vocab)+1#unknown \n",
    "index_sos=len(ft_vocab)+2#start of sentence\n",
    "\n",
    "\n",
    "ft_vocab.insert_token(\"<pad>\",index_pad)\n",
    "ft_vocab.insert_token(\"<unk>\",index_unk)\n",
    "ft_vocab.insert_token(\"<s>\",index_sos)\n",
    "\n",
    "ft_vocab.set_default_index(index_unk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qFqnjIxiy6Y"
   },
   "source": [
    "dl data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Q_vd-Svbizil"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/Ellana42/style_transfer_proust/main/datasets/dataset.csv\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MMlGeWSAi0ze"
   },
   "outputs": [],
   "source": [
    "tokenizer_function=functools.partial(tokenize_sentence,tokenizer=tokenizer)\n",
    "df = pd.read_csv('dataset.csv', sep='|', index_col=0).dropna()\n",
    "dataset = text_Dataset(\"\", tokenizer_function, ft_vocab, df=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio, val_ratio, test_ratio = (0.8, 0.1, 0.1)\n",
    "train_size = int(len(dataset) * train_ratio)\n",
    "val_size = int(len(dataset) * val_ratio)\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train, val, test = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ZXaVev4bi8if"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test, batch_size=batch_size, shuffle = True)\n",
    "val_dataloader = DataLoader(val, batch_size=batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SprN-0qqjenA"
   },
   "source": [
    "get one batch of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FNs7QwpXi_Ti"
   },
   "outputs": [],
   "source": [
    "#input_idx ,labels = next(iter(test_dataload))\n",
    "#labels = labels.view(-1,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEhbT51KkTJR"
   },
   "source": [
    "# running fct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYDZBSN6jQ8N"
   },
   "source": [
    "parameter dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7GP2kFmcjE4B",
    "outputId": "e2a6ed9b-47fa-412d-ebcb-5087ea26d001"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'style': {'batch_size': 64,\n",
       "  'label_smoothing_sigma': 0.1,\n",
       "  'lr': 0.01,\n",
       "  'style_embed_dim': 128,\n",
       "  'max_generated_sequence_len': 15,\n",
       "  'eos_idx': 0,\n",
       "  'vocab_size': 30756,\n",
       "  'embed_size': 300,\n",
       "  'self.sos_idx': 30758},\n",
       " 'autoencoder': {'encoder': {'nb_layers': 1,\n",
       "   'embed_size': 300,\n",
       "   'hidden_dim': 258},\n",
       "  'decoder': {'nb_layers': 1, 'embed_size': 300, 'hidden_dim': 258}}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_dict={}\n",
    "batch_size=64\n",
    "style_embed_dim = 128 # dim of the Latent representation of the style variable\n",
    "max_generated_sequence_len = 15\n",
    "vocab_size = len(ft_vocab)-3 # since pretrained vectors do not have embedding for <s> <pad> and <unk> which are in vocabulary\n",
    "hidden_dim=258\n",
    "embed_size = 300\n",
    "lr = 0.01\n",
    "\n",
    "style_dict={\"batch_size\" :batch_size,\"label_smoothing_sigma\" : 0.1, \"lr\": lr, \"style_embed_dim\" : style_embed_dim, \"max_generated_sequence_len\":max_generated_sequence_len, \"eos_idx\":0, \"vocab_size\":vocab_size, \"embed_size\":embed_size, \"self.sos_idx\":index_sos}\n",
    "\n",
    "encoder_dict={\"nb_layers\":1, \"embed_size\":embed_size,\"hidden_dim\":hidden_dim}\n",
    "decoder_dict={\"nb_layers\":1, \"embed_size\":embed_size,\"hidden_dim\":hidden_dim}\n",
    "\n",
    "autoencoder_dict={\"encoder\":encoder_dict,\"decoder\":decoder_dict}\n",
    "\n",
    "parameters_dict[\"style\"]=style_dict\n",
    "parameters_dict[\"autoencoder\"]=autoencoder_dict\n",
    "parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MoIdqFfojcnw"
   },
   "outputs": [],
   "source": [
    "style_model = style_transfer(parameters_dict,pretrained_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCoCL4h2moxG"
   },
   "source": [
    "Predict sentences : careful this function only works in eval mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VK3K08WTkYHD",
    "outputId": "c553378d-2ba8-47f3-c2af-7d359dd5f4d4"
   },
   "outputs": [],
   "source": [
    "#style_model.predict(input_idx,labels,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DN0dVDSimwut"
   },
   "source": [
    "train on batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WdFTtc5yk55j",
    "outputId": "d89e2701-ef3e-4945-b9bc-ed405a3eb6cf"
   },
   "outputs": [],
   "source": [
    "#for _ in range(20):\n",
    "    #style_model.train_batch(input_idx, labels, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=4, evaluation=False):\n",
    "    print(\"Start training...\\n\")\n",
    "    batch_counts = 0\n",
    "    for epoch_i in range(epochs):\n",
    "        print(\n",
    "        f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\"\n",
    "        )\n",
    "        print(\"-\" * 70)\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "        step, total_loss, batch_loss, batch_count = 0, 0, 0, 0\n",
    "        for _ in range(len(train_dataloader)):\n",
    "            try:\n",
    "                input_idx, labels = next(iter(train_dataloader))\n",
    "            except ValueError:\n",
    "                continue\n",
    "            labels = labels.view(-1,1,1)\n",
    "            batch_count += 1\n",
    "            step += 1\n",
    "            loss = style_model.train_batch(input_idx, labels, 0)\n",
    "            batch_loss += loss\n",
    "            total_loss += loss\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "        if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "            time_elapsed = time.time() - t0_batch\n",
    "            print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "            batch_loss, batch_count = 0, 0\n",
    "            t0_batch = time.time()\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(\"-\" * 70)\n",
    "        if (evaluation==True):\n",
    "            total_loss = 0\n",
    "            n_batch = 0\n",
    "            for input_idx ,labels in val_dataloader:\n",
    "                labels = labels.view(-1,1,1)\n",
    "                n_batch += 1\n",
    "                total_loss += style_model.eval_batch(input_idx, labels)\n",
    "                time_elapsed = time.time() - t0_epoch\n",
    "            val_loss = total_loss/len(val_dataloader)\n",
    "            print(\n",
    "                f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {'-':^9} | {time_elapsed:^9.2f}\"\n",
    "            )\n",
    "            print(\"-\" * 70)\n",
    "            print(\"\\n\")\n",
    "            print(\"Training complete!\")\n",
    "            return (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_249020/1257888086.py:330: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = self.batch_loss(gen_input, torch.tensor(input_idx_tokens), labels)\n",
      "/tmp/ipykernel_249020/1257888086.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels=torch.tensor(labels).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction loss is 10.350297927856445\n",
      "reconstruction loss is 9.364091873168945\n",
      "reconstruction loss is 6.93774938583374\n",
      "reconstruction loss is 6.1809773445129395\n",
      "reconstruction loss is 6.308010101318359\n",
      "reconstruction loss is 6.238048076629639\n",
      "reconstruction loss is 6.062816619873047\n",
      "reconstruction loss is 6.008533954620361\n",
      "reconstruction loss is 5.9430365562438965\n",
      "reconstruction loss is 5.594308376312256\n",
      "reconstruction loss is 5.566992282867432\n",
      "reconstruction loss is 5.4683356285095215\n",
      "reconstruction loss is 5.109234809875488\n",
      "reconstruction loss is 5.0404438972473145\n",
      "reconstruction loss is 5.085691928863525\n",
      "reconstruction loss is 4.822578430175781\n",
      "reconstruction loss is 4.785800933837891\n",
      "reconstruction loss is 4.976200580596924\n",
      "reconstruction loss is 4.880957126617432\n",
      "reconstruction loss is 4.865368366241455\n",
      "reconstruction loss is 4.414326190948486\n",
      "reconstruction loss is 4.483524799346924\n",
      "reconstruction loss is 4.349912166595459\n",
      "reconstruction loss is 4.267012119293213\n",
      "reconstruction loss is 4.043481826782227\n",
      "reconstruction loss is 4.0731425285339355\n",
      "reconstruction loss is 3.809054136276245\n",
      "reconstruction loss is 4.288686275482178\n",
      "reconstruction loss is 3.998126983642578\n",
      "reconstruction loss is 3.607062339782715\n",
      "reconstruction loss is 3.6046793460845947\n",
      "reconstruction loss is 3.481947660446167\n",
      "reconstruction loss is 3.4556567668914795\n",
      "reconstruction loss is 3.4747838973999023\n",
      "reconstruction loss is 3.575749397277832\n",
      "reconstruction loss is 3.430506467819214\n",
      "reconstruction loss is 3.3371055126190186\n",
      "reconstruction loss is 3.349790573120117\n",
      "reconstruction loss is 3.0282161235809326\n",
      "reconstruction loss is 3.3067476749420166\n",
      "reconstruction loss is 3.2101142406463623\n",
      "reconstruction loss is 2.9757683277130127\n",
      "reconstruction loss is 3.228684663772583\n",
      "reconstruction loss is 2.8841116428375244\n",
      "reconstruction loss is 3.1914453506469727\n",
      "reconstruction loss is 3.2737643718719482\n",
      "reconstruction loss is 3.0918385982513428\n",
      "reconstruction loss is 2.813966751098633\n",
      "reconstruction loss is 2.907865524291992\n",
      "reconstruction loss is 2.8395516872406006\n",
      "reconstruction loss is 2.9395523071289062\n",
      "reconstruction loss is 2.8597195148468018\n",
      "reconstruction loss is 2.6716673374176025\n",
      "reconstruction loss is 2.7144763469696045\n",
      "reconstruction loss is 3.1239213943481445\n",
      "reconstruction loss is 2.9156105518341064\n",
      "reconstruction loss is 2.920802116394043\n",
      "reconstruction loss is 2.957353353500366\n",
      "reconstruction loss is 2.8051230907440186\n",
      "reconstruction loss is 2.8794450759887695\n",
      "reconstruction loss is 2.8318068981170654\n",
      "reconstruction loss is 2.5063886642456055\n",
      "reconstruction loss is 2.7409207820892334\n",
      "reconstruction loss is 2.3575613498687744\n",
      "reconstruction loss is 2.465240955352783\n",
      "reconstruction loss is 2.363616466522217\n",
      "reconstruction loss is 2.7911837100982666\n",
      "reconstruction loss is 2.589462995529175\n",
      "reconstruction loss is 2.5558981895446777\n",
      "reconstruction loss is 2.36738920211792\n",
      "reconstruction loss is 2.483389377593994\n",
      "reconstruction loss is 2.0808751583099365\n",
      "reconstruction loss is 2.2039971351623535\n",
      "reconstruction loss is 2.2789242267608643\n",
      "reconstruction loss is 2.3630917072296143\n",
      "reconstruction loss is 2.131864309310913\n",
      "reconstruction loss is 2.212292432785034\n",
      "reconstruction loss is 2.357858419418335\n",
      "reconstruction loss is 2.074808359146118\n",
      "reconstruction loss is 2.312988042831421\n",
      "reconstruction loss is 2.214655876159668\n",
      "reconstruction loss is 2.208800792694092\n",
      "reconstruction loss is 2.10111403465271\n",
      "reconstruction loss is 2.1746442317962646\n",
      "reconstruction loss is 2.1783151626586914\n",
      "reconstruction loss is 2.4688003063201904\n",
      "reconstruction loss is 2.261716842651367\n",
      "reconstruction loss is 1.912275791168213\n",
      "reconstruction loss is 1.7936749458312988\n",
      "reconstruction loss is 2.243518590927124\n",
      "reconstruction loss is 1.799418330192566\n",
      "reconstruction loss is 1.877865195274353\n",
      "reconstruction loss is 1.9787230491638184\n",
      "reconstruction loss is 2.048097848892212\n",
      "reconstruction loss is 1.9787904024124146\n",
      "reconstruction loss is 2.020167350769043\n",
      "reconstruction loss is 1.7792819738388062\n",
      "reconstruction loss is 1.6878937482833862\n",
      "reconstruction loss is 2.007354497909546\n",
      "reconstruction loss is 1.7182159423828125\n",
      "reconstruction loss is 1.5477451086044312\n",
      "reconstruction loss is 1.8939589262008667\n",
      "reconstruction loss is 1.674230933189392\n",
      "reconstruction loss is 1.816147804260254\n",
      "reconstruction loss is 1.9653569459915161\n",
      "reconstruction loss is 1.5814685821533203\n",
      "reconstruction loss is 1.6406283378601074\n",
      "reconstruction loss is 1.7497261762619019\n",
      "reconstruction loss is 1.8810011148452759\n",
      "reconstruction loss is 1.6186094284057617\n",
      "reconstruction loss is 1.6895452737808228\n",
      "reconstruction loss is 1.6087461709976196\n",
      "reconstruction loss is 1.596491813659668\n",
      "reconstruction loss is 1.6605790853500366\n",
      "reconstruction loss is 1.53691565990448\n",
      "reconstruction loss is 1.509565830230713\n",
      "reconstruction loss is 1.6886014938354492\n",
      "reconstruction loss is 1.729794979095459\n",
      "reconstruction loss is 1.4075795412063599\n",
      "reconstruction loss is 1.8477325439453125\n",
      "reconstruction loss is 1.6239957809448242\n",
      "reconstruction loss is 1.4187113046646118\n",
      "reconstruction loss is 1.5253385305404663\n",
      "reconstruction loss is 1.3643540143966675\n",
      "reconstruction loss is 1.5712569952011108\n",
      "reconstruction loss is 1.4588438272476196\n",
      "reconstruction loss is 1.48883855342865\n",
      "reconstruction loss is 1.4853254556655884\n",
      "reconstruction loss is 1.6183983087539673\n",
      "reconstruction loss is 1.4816490411758423\n",
      "reconstruction loss is 1.5095654726028442\n",
      "reconstruction loss is 1.4332151412963867\n",
      "reconstruction loss is 1.5535354614257812\n",
      "reconstruction loss is 1.2189738750457764\n",
      "reconstruction loss is 1.2918862104415894\n",
      "reconstruction loss is 1.651537299156189\n",
      "reconstruction loss is 1.7003320455551147\n",
      "reconstruction loss is 1.3176864385604858\n",
      "reconstruction loss is 1.5720361471176147\n",
      "reconstruction loss is 1.2458847761154175\n",
      "reconstruction loss is 1.3751983642578125\n",
      "reconstruction loss is 1.2155989408493042\n",
      "reconstruction loss is 1.3802660703659058\n",
      "reconstruction loss is 1.3717740774154663\n",
      "reconstruction loss is 1.4723033905029297\n",
      "reconstruction loss is 1.402577519416809\n",
      "reconstruction loss is 1.3821659088134766\n",
      "reconstruction loss is 1.1012380123138428\n",
      "reconstruction loss is 1.186705470085144\n",
      "reconstruction loss is 1.2734538316726685\n",
      "reconstruction loss is 1.3526297807693481\n",
      "reconstruction loss is 1.2493456602096558\n",
      "reconstruction loss is 1.2302067279815674\n",
      "reconstruction loss is 1.349749207496643\n",
      "reconstruction loss is 1.2841906547546387\n",
      "reconstruction loss is 1.4821757078170776\n",
      "reconstruction loss is 1.2890583276748657\n",
      "reconstruction loss is 1.1789175271987915\n",
      "reconstruction loss is 1.4449273347854614\n",
      "reconstruction loss is 1.484926700592041\n",
      "reconstruction loss is 1.4628747701644897\n",
      "reconstruction loss is 1.249976634979248\n",
      "reconstruction loss is 0.9128279089927673\n",
      "reconstruction loss is 1.3982930183410645\n",
      "reconstruction loss is 1.2035702466964722\n",
      "reconstruction loss is 1.3624969720840454\n",
      "reconstruction loss is 1.1740449666976929\n",
      "reconstruction loss is 1.2907644510269165\n",
      "reconstruction loss is 1.3092609643936157\n",
      "reconstruction loss is 1.3253718614578247\n",
      "reconstruction loss is 1.1702983379364014\n",
      "reconstruction loss is 1.044655680656433\n",
      "reconstruction loss is 1.2622313499450684\n",
      "reconstruction loss is 1.120102882385254\n",
      "reconstruction loss is 1.393762230873108\n",
      "reconstruction loss is 1.2653173208236694\n",
      "reconstruction loss is 1.0891786813735962\n",
      "reconstruction loss is 1.028171420097351\n",
      "reconstruction loss is 0.9264464974403381\n",
      "reconstruction loss is 1.1260284185409546\n",
      "reconstruction loss is 1.1794852018356323\n",
      "reconstruction loss is 1.0150130987167358\n",
      "reconstruction loss is 1.0820825099945068\n",
      "reconstruction loss is 1.284006953239441\n",
      "reconstruction loss is 1.1333037614822388\n",
      "reconstruction loss is 1.0589529275894165\n",
      "reconstruction loss is 1.1969579458236694\n",
      "reconstruction loss is 0.9924228191375732\n",
      "reconstruction loss is 0.9064362049102783\n",
      "reconstruction loss is 1.0286911725997925\n",
      "reconstruction loss is 0.9364168643951416\n",
      "reconstruction loss is 1.06670081615448\n",
      "reconstruction loss is 1.0640252828598022\n",
      "reconstruction loss is 0.9928211569786072\n",
      "reconstruction loss is 1.0855835676193237\n",
      "reconstruction loss is 0.988857090473175\n",
      "reconstruction loss is 1.0426563024520874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction loss is 0.9614198803901672\n",
      "reconstruction loss is 1.0729657411575317\n",
      "reconstruction loss is 1.0710725784301758\n",
      "reconstruction loss is 1.0396872758865356\n",
      "reconstruction loss is 0.9931395649909973\n",
      "reconstruction loss is 0.8480016589164734\n",
      "reconstruction loss is 0.9688658714294434\n",
      "reconstruction loss is 1.0132614374160767\n",
      "reconstruction loss is 0.9964383244514465\n",
      "reconstruction loss is 1.001675009727478\n",
      "reconstruction loss is 1.013398289680481\n",
      "reconstruction loss is 1.0171486139297485\n",
      "reconstruction loss is 0.8925380110740662\n",
      "reconstruction loss is 1.097469687461853\n",
      "reconstruction loss is 0.912207305431366\n",
      "reconstruction loss is 0.8431041836738586\n",
      "reconstruction loss is 0.8899450302124023\n",
      "reconstruction loss is 0.802812397480011\n",
      "reconstruction loss is 0.9011675715446472\n",
      "reconstruction loss is 0.8143156170845032\n",
      "reconstruction loss is 0.9549314975738525\n",
      "reconstruction loss is 1.1809768676757812\n",
      "reconstruction loss is 0.9548759460449219\n",
      "reconstruction loss is 0.8922253251075745\n",
      "reconstruction loss is 1.096968650817871\n",
      "reconstruction loss is 0.9054935574531555\n",
      "reconstruction loss is 0.9624900221824646\n",
      "reconstruction loss is 0.8274755477905273\n",
      "reconstruction loss is 0.8280721306800842\n",
      "reconstruction loss is 0.806601345539093\n",
      "reconstruction loss is 1.0655137300491333\n",
      "reconstruction loss is 0.7708067297935486\n",
      "reconstruction loss is 0.8564975261688232\n",
      "reconstruction loss is 0.8396108150482178\n",
      "reconstruction loss is 0.9147272109985352\n",
      "reconstruction loss is 0.7135884165763855\n",
      "reconstruction loss is 0.942413330078125\n",
      "reconstruction loss is 0.9907392859458923\n",
      "reconstruction loss is 0.8904392719268799\n",
      "reconstruction loss is 0.9108867049217224\n",
      "reconstruction loss is 0.9065814018249512\n",
      "reconstruction loss is 0.6692727208137512\n",
      "reconstruction loss is 0.6500865817070007\n",
      "reconstruction loss is 0.8375487327575684\n",
      "reconstruction loss is 0.6891823410987854\n",
      "reconstruction loss is 0.8231766819953918\n",
      "reconstruction loss is 0.8324452042579651\n",
      "reconstruction loss is 0.8760688304901123\n",
      "reconstruction loss is 0.6299320459365845\n",
      "----------------------------------------------------------------------\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "reconstruction loss is 0.9008312225341797\n",
      "reconstruction loss is 0.9345846176147461\n",
      "reconstruction loss is 0.8692772388458252\n",
      "reconstruction loss is 0.8894034028053284\n",
      "reconstruction loss is 1.091560959815979\n",
      "reconstruction loss is 0.8651614189147949\n",
      "reconstruction loss is 0.8964789509773254\n",
      "reconstruction loss is 0.7617035508155823\n",
      "reconstruction loss is 0.7352592945098877\n",
      "reconstruction loss is 0.7848072648048401\n",
      "reconstruction loss is 0.7763918042182922\n",
      "reconstruction loss is 0.7671940922737122\n",
      "reconstruction loss is 0.887850284576416\n",
      "reconstruction loss is 0.8369216322898865\n",
      "reconstruction loss is 0.8817104697227478\n",
      "reconstruction loss is 0.8660159111022949\n",
      "reconstruction loss is 0.8014817833900452\n",
      "reconstruction loss is 1.0081429481506348\n",
      "reconstruction loss is 0.701255738735199\n",
      "reconstruction loss is 0.9163870811462402\n",
      "reconstruction loss is 0.8567461371421814\n",
      "reconstruction loss is 0.9041202068328857\n",
      "reconstruction loss is 0.7001941204071045\n",
      "reconstruction loss is 0.7669243216514587\n",
      "reconstruction loss is 0.7812281250953674\n",
      "reconstruction loss is 0.7688924670219421\n",
      "reconstruction loss is 0.7055328488349915\n",
      "reconstruction loss is 1.0370806455612183\n",
      "reconstruction loss is 0.7284374833106995\n",
      "reconstruction loss is 0.6255995631217957\n",
      "reconstruction loss is 0.8747681975364685\n",
      "reconstruction loss is 0.6021592020988464\n",
      "reconstruction loss is 0.6815789341926575\n",
      "reconstruction loss is 0.7627003192901611\n",
      "reconstruction loss is 0.6717495918273926\n",
      "reconstruction loss is 0.7648676037788391\n",
      "reconstruction loss is 0.8899264335632324\n",
      "reconstruction loss is 0.8672387599945068\n",
      "reconstruction loss is 0.7286260724067688\n",
      "reconstruction loss is 0.766517162322998\n",
      "reconstruction loss is 0.7406832575798035\n",
      "reconstruction loss is 0.678165853023529\n",
      "reconstruction loss is 0.5732758641242981\n",
      "reconstruction loss is 0.8800194263458252\n",
      "reconstruction loss is 0.6520490050315857\n",
      "reconstruction loss is 0.6922640204429626\n",
      "reconstruction loss is 0.7618419528007507\n",
      "reconstruction loss is 0.7005682587623596\n",
      "reconstruction loss is 0.7038957476615906\n",
      "reconstruction loss is 0.6316370368003845\n",
      "reconstruction loss is 0.6988001465797424\n",
      "reconstruction loss is 0.7723445296287537\n",
      "reconstruction loss is 0.6645491719245911\n",
      "reconstruction loss is 0.8076379895210266\n",
      "reconstruction loss is 0.7379348278045654\n",
      "reconstruction loss is 0.5638662576675415\n",
      "reconstruction loss is 0.7303232550621033\n",
      "reconstruction loss is 0.6661073565483093\n",
      "reconstruction loss is 0.8273127675056458\n",
      "reconstruction loss is 0.6831527352333069\n",
      "reconstruction loss is 0.8045420050621033\n",
      "reconstruction loss is 0.5600370764732361\n",
      "reconstruction loss is 0.6494461894035339\n",
      "reconstruction loss is 0.705099880695343\n",
      "reconstruction loss is 0.6499338150024414\n",
      "reconstruction loss is 0.827614963054657\n",
      "reconstruction loss is 0.6438037753105164\n",
      "reconstruction loss is 0.5879668593406677\n",
      "reconstruction loss is 0.609297513961792\n",
      "reconstruction loss is 0.49039986729621887\n",
      "reconstruction loss is 0.6563461422920227\n",
      "reconstruction loss is 0.6780977249145508\n",
      "reconstruction loss is 0.7489927411079407\n",
      "reconstruction loss is 0.43539202213287354\n",
      "reconstruction loss is 0.7424712777137756\n",
      "reconstruction loss is 0.6757821440696716\n",
      "reconstruction loss is 0.6020908355712891\n",
      "reconstruction loss is 0.47249212861061096\n",
      "reconstruction loss is 0.511926531791687\n",
      "reconstruction loss is 0.7086031436920166\n",
      "reconstruction loss is 0.5033577680587769\n",
      "reconstruction loss is 0.6564788222312927\n",
      "reconstruction loss is 0.7945166230201721\n",
      "reconstruction loss is 0.5947247743606567\n",
      "reconstruction loss is 0.7337984442710876\n",
      "reconstruction loss is 0.5923489332199097\n",
      "reconstruction loss is 0.5770788788795471\n",
      "reconstruction loss is 0.6726117730140686\n",
      "reconstruction loss is 0.6625590324401855\n",
      "reconstruction loss is 0.535955011844635\n",
      "reconstruction loss is 0.8506167531013489\n",
      "reconstruction loss is 0.7317718863487244\n",
      "reconstruction loss is 0.6750285029411316\n",
      "reconstruction loss is 0.59940105676651\n",
      "reconstruction loss is 0.5811718106269836\n",
      "reconstruction loss is 0.4636278450489044\n",
      "reconstruction loss is 0.628682553768158\n",
      "reconstruction loss is 0.5494316816329956\n",
      "reconstruction loss is 0.7507477402687073\n",
      "reconstruction loss is 0.5874444842338562\n",
      "reconstruction loss is 0.46254900097846985\n",
      "reconstruction loss is 0.5217216610908508\n",
      "reconstruction loss is 0.7166923880577087\n",
      "reconstruction loss is 0.5836725831031799\n",
      "reconstruction loss is 0.668613851070404\n",
      "reconstruction loss is 0.48620638251304626\n",
      "reconstruction loss is 0.5510285496711731\n",
      "reconstruction loss is 0.6975557208061218\n",
      "reconstruction loss is 0.7335062026977539\n",
      "reconstruction loss is 0.4125044643878937\n",
      "reconstruction loss is 0.6433317065238953\n",
      "reconstruction loss is 0.7360701560974121\n",
      "reconstruction loss is 0.5820691585540771\n",
      "reconstruction loss is 0.5860337018966675\n",
      "reconstruction loss is 0.605769693851471\n",
      "reconstruction loss is 0.5583906769752502\n",
      "reconstruction loss is 0.5538459420204163\n",
      "reconstruction loss is 0.632580578327179\n",
      "reconstruction loss is 0.5917233824729919\n",
      "reconstruction loss is 0.5009613037109375\n",
      "reconstruction loss is 0.5847012996673584\n",
      "reconstruction loss is 0.5234273076057434\n",
      "reconstruction loss is 0.39726778864860535\n",
      "reconstruction loss is 0.5981587767601013\n",
      "reconstruction loss is 0.720533549785614\n",
      "reconstruction loss is 0.35169923305511475\n",
      "reconstruction loss is 0.46542593836784363\n",
      "reconstruction loss is 0.535837709903717\n",
      "reconstruction loss is 0.5927274227142334\n",
      "reconstruction loss is 0.7138850092887878\n",
      "reconstruction loss is 0.6105647087097168\n",
      "reconstruction loss is 0.5226348638534546\n",
      "reconstruction loss is 0.468638151884079\n",
      "reconstruction loss is 0.6086480021476746\n",
      "reconstruction loss is 0.4453199803829193\n",
      "reconstruction loss is 0.5185077786445618\n",
      "reconstruction loss is 0.38800013065338135\n",
      "reconstruction loss is 0.6416772603988647\n",
      "reconstruction loss is 0.4228745996952057\n",
      "reconstruction loss is 0.6041259169578552\n",
      "reconstruction loss is 0.5125259757041931\n",
      "reconstruction loss is 0.5929579138755798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction loss is 0.5038682818412781\n",
      "reconstruction loss is 0.4514045715332031\n",
      "reconstruction loss is 0.5343685150146484\n",
      "reconstruction loss is 0.418082594871521\n",
      "reconstruction loss is 0.5423470139503479\n",
      "reconstruction loss is 0.5704240798950195\n",
      "reconstruction loss is 0.47047653794288635\n",
      "reconstruction loss is 0.4745709002017975\n",
      "reconstruction loss is 0.4634797275066376\n",
      "reconstruction loss is 0.5417660474777222\n",
      "reconstruction loss is 0.48550212383270264\n",
      "reconstruction loss is 0.5578956007957458\n",
      "reconstruction loss is 0.44810226559638977\n",
      "reconstruction loss is 0.43913665413856506\n",
      "reconstruction loss is 0.3916504681110382\n",
      "reconstruction loss is 0.42297324538230896\n",
      "reconstruction loss is 0.5999931693077087\n",
      "reconstruction loss is 0.5318906307220459\n",
      "reconstruction loss is 0.5041128993034363\n",
      "reconstruction loss is 0.4773534834384918\n",
      "reconstruction loss is 0.5862573981285095\n",
      "reconstruction loss is 0.6700155735015869\n",
      "reconstruction loss is 0.6466161608695984\n",
      "reconstruction loss is 0.5092400908470154\n",
      "reconstruction loss is 0.6086557507514954\n",
      "reconstruction loss is 0.441649466753006\n",
      "reconstruction loss is 0.4988604784011841\n",
      "reconstruction loss is 0.5080705881118774\n",
      "reconstruction loss is 0.3015134632587433\n",
      "reconstruction loss is 0.6899778842926025\n",
      "reconstruction loss is 0.5861479640007019\n",
      "reconstruction loss is 0.5627875924110413\n",
      "reconstruction loss is 0.6749226450920105\n",
      "reconstruction loss is 0.44158604741096497\n",
      "reconstruction loss is 0.5193832516670227\n",
      "reconstruction loss is 0.46743372082710266\n",
      "reconstruction loss is 0.3898593485355377\n",
      "reconstruction loss is 0.5202199220657349\n",
      "reconstruction loss is 0.6274048089981079\n",
      "reconstruction loss is 0.648419201374054\n",
      "reconstruction loss is 0.4158230125904083\n",
      "reconstruction loss is 0.5323001146316528\n",
      "reconstruction loss is 0.39389297366142273\n",
      "reconstruction loss is 0.4998907744884491\n",
      "reconstruction loss is 0.5265557169914246\n",
      "reconstruction loss is 0.6220086216926575\n",
      "reconstruction loss is 0.6906249523162842\n",
      "reconstruction loss is 0.5030822157859802\n",
      "reconstruction loss is 0.4968513250350952\n",
      "reconstruction loss is 0.39396119117736816\n",
      "reconstruction loss is 0.5574585795402527\n",
      "reconstruction loss is 0.393078088760376\n",
      "reconstruction loss is 0.49474677443504333\n",
      "reconstruction loss is 0.5370588898658752\n",
      "reconstruction loss is 0.38573166728019714\n",
      "reconstruction loss is 0.5251855254173279\n",
      "reconstruction loss is 0.37290874123573303\n",
      "reconstruction loss is 0.6032248139381409\n",
      "reconstruction loss is 0.26151537895202637\n",
      "reconstruction loss is 0.5223268866539001\n",
      "reconstruction loss is 0.5512345433235168\n",
      "reconstruction loss is 0.500141441822052\n",
      "reconstruction loss is 0.4253568947315216\n",
      "reconstruction loss is 0.4045243561267853\n",
      "reconstruction loss is 0.45795848965644836\n",
      "reconstruction loss is 0.5006154179573059\n",
      "reconstruction loss is 0.552761435508728\n",
      "reconstruction loss is 0.3615396320819855\n",
      "reconstruction loss is 0.540132462978363\n",
      "reconstruction loss is 0.7121017575263977\n",
      "reconstruction loss is 0.5690171718597412\n",
      "reconstruction loss is 0.495670348405838\n",
      "reconstruction loss is 0.5259025692939758\n",
      "reconstruction loss is 0.45467206835746765\n",
      "reconstruction loss is 0.5094594955444336\n",
      "reconstruction loss is 0.6102980971336365\n",
      "reconstruction loss is 0.5372915863990784\n",
      "reconstruction loss is 0.5469614863395691\n",
      "reconstruction loss is 0.5579574108123779\n",
      "reconstruction loss is 0.48397716879844666\n",
      "reconstruction loss is 0.5152931809425354\n",
      "reconstruction loss is 0.3273230195045471\n",
      "reconstruction loss is 0.5055047869682312\n",
      "reconstruction loss is 0.3118495047092438\n",
      "reconstruction loss is 0.49343129992485046\n",
      "reconstruction loss is 0.39978358149528503\n",
      "reconstruction loss is 0.5736140608787537\n",
      "reconstruction loss is 0.6104260087013245\n",
      "reconstruction loss is 0.5358479619026184\n",
      "reconstruction loss is 0.49255993962287903\n",
      "reconstruction loss is 0.4457673132419586\n",
      "reconstruction loss is 0.4543417990207672\n",
      "reconstruction loss is 0.3433718681335449\n",
      "reconstruction loss is 0.4929044246673584\n",
      "reconstruction loss is 0.4110840857028961\n",
      "reconstruction loss is 0.30699875950813293\n",
      "reconstruction loss is 0.39881816506385803\n",
      "reconstruction loss is 0.3850249946117401\n",
      "reconstruction loss is 0.3445194661617279\n",
      "reconstruction loss is 0.3637782037258148\n",
      "reconstruction loss is 0.36283135414123535\n",
      "reconstruction loss is 0.45111843943595886\n",
      "----------------------------------------------------------------------\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "reconstruction loss is 0.4230927526950836\n",
      "reconstruction loss is 0.5069669485092163\n",
      "reconstruction loss is 0.5499855875968933\n",
      "reconstruction loss is 0.5577600002288818\n",
      "reconstruction loss is 0.4863988161087036\n",
      "reconstruction loss is 0.3297710716724396\n",
      "reconstruction loss is 0.6383684873580933\n",
      "reconstruction loss is 0.27015992999076843\n",
      "reconstruction loss is 0.40852925181388855\n",
      "reconstruction loss is 0.5952518582344055\n",
      "reconstruction loss is 0.3629898726940155\n",
      "reconstruction loss is 0.37373682856559753\n",
      "reconstruction loss is 0.3598676025867462\n",
      "reconstruction loss is 0.4423436224460602\n",
      "reconstruction loss is 0.37099143862724304\n",
      "reconstruction loss is 0.4917597472667694\n",
      "reconstruction loss is 0.42881515622138977\n",
      "reconstruction loss is 0.39689990878105164\n",
      "reconstruction loss is 0.44429364800453186\n",
      "reconstruction loss is 0.29797598719596863\n",
      "reconstruction loss is 0.3954903781414032\n",
      "reconstruction loss is 0.3989208936691284\n",
      "reconstruction loss is 0.5339725017547607\n",
      "reconstruction loss is 0.36789146065711975\n",
      "reconstruction loss is 0.3188036382198334\n",
      "reconstruction loss is 0.45775875449180603\n",
      "reconstruction loss is 0.49456140398979187\n",
      "reconstruction loss is 0.23583431541919708\n",
      "reconstruction loss is 0.347507506608963\n",
      "reconstruction loss is 0.5252664685249329\n",
      "reconstruction loss is 0.5060714483261108\n",
      "reconstruction loss is 0.3657430410385132\n",
      "reconstruction loss is 0.3258607089519501\n",
      "reconstruction loss is 0.3655354678630829\n",
      "reconstruction loss is 0.22874487936496735\n",
      "reconstruction loss is 0.2948349416255951\n",
      "reconstruction loss is 0.36549481749534607\n",
      "reconstruction loss is 0.44098278880119324\n",
      "reconstruction loss is 0.3237592577934265\n",
      "reconstruction loss is 0.5472247004508972\n",
      "reconstruction loss is 0.577671468257904\n",
      "reconstruction loss is 0.36737295985221863\n",
      "reconstruction loss is 0.4826434552669525\n",
      "reconstruction loss is 0.4000673294067383\n",
      "reconstruction loss is 0.4144981801509857\n",
      "reconstruction loss is 0.4402294158935547\n",
      "reconstruction loss is 0.4244481325149536\n",
      "reconstruction loss is 0.3548969328403473\n",
      "reconstruction loss is 0.392895370721817\n",
      "reconstruction loss is 0.420985609292984\n",
      "reconstruction loss is 0.2835423946380615\n",
      "reconstruction loss is 0.40888169407844543\n",
      "reconstruction loss is 0.4730932414531708\n",
      "reconstruction loss is 0.16441434621810913\n",
      "reconstruction loss is 0.5113551020622253\n",
      "reconstruction loss is 0.4015083312988281\n",
      "reconstruction loss is 0.4001329243183136\n",
      "reconstruction loss is 0.4177686870098114\n",
      "reconstruction loss is 0.3894587457180023\n",
      "reconstruction loss is 0.28497186303138733\n",
      "reconstruction loss is 0.2815514802932739\n",
      "reconstruction loss is 0.3667096197605133\n",
      "reconstruction loss is 0.3298923671245575\n",
      "reconstruction loss is 0.2618083953857422\n",
      "reconstruction loss is 0.3541674315929413\n",
      "reconstruction loss is 0.25621771812438965\n",
      "reconstruction loss is 0.34117797017097473\n",
      "reconstruction loss is 0.5364381670951843\n",
      "reconstruction loss is 0.4175635874271393\n",
      "reconstruction loss is 0.3573110103607178\n",
      "reconstruction loss is 0.3236541450023651\n",
      "reconstruction loss is 0.3344673216342926\n",
      "reconstruction loss is 0.28769782185554504\n",
      "reconstruction loss is 0.23047752678394318\n",
      "reconstruction loss is 0.42573416233062744\n",
      "reconstruction loss is 0.36008918285369873\n",
      "reconstruction loss is 0.4012352526187897\n",
      "reconstruction loss is 0.4016512930393219\n",
      "reconstruction loss is 0.4616137444972992\n",
      "reconstruction loss is 0.41424253582954407\n",
      "reconstruction loss is 0.39974597096443176\n",
      "reconstruction loss is 0.35090136528015137\n",
      "reconstruction loss is 0.27930721640586853\n",
      "reconstruction loss is 0.3293418288230896\n",
      "reconstruction loss is 0.34611788392066956\n",
      "reconstruction loss is 0.38646888732910156\n",
      "reconstruction loss is 0.4380815029144287\n",
      "reconstruction loss is 0.37199974060058594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction loss is 0.31084996461868286\n",
      "reconstruction loss is 0.4211095869541168\n",
      "reconstruction loss is 0.4837020933628082\n",
      "reconstruction loss is 0.5494161248207092\n",
      "reconstruction loss is 0.37441158294677734\n",
      "reconstruction loss is 0.5529966950416565\n",
      "reconstruction loss is 0.48389020562171936\n",
      "reconstruction loss is 0.46776703000068665\n",
      "reconstruction loss is 0.4122180640697479\n",
      "reconstruction loss is 0.377361923456192\n",
      "reconstruction loss is 0.2720414400100708\n",
      "reconstruction loss is 0.2696150839328766\n",
      "reconstruction loss is 0.23815983533859253\n",
      "reconstruction loss is 0.44866621494293213\n",
      "reconstruction loss is 0.33827534317970276\n",
      "reconstruction loss is 0.45185065269470215\n",
      "reconstruction loss is 0.4608032703399658\n",
      "reconstruction loss is 0.26822131872177124\n",
      "reconstruction loss is 0.3953976631164551\n",
      "reconstruction loss is 0.3865283727645874\n",
      "reconstruction loss is 0.34800246357917786\n",
      "reconstruction loss is 0.43830832839012146\n",
      "reconstruction loss is 0.33316585421562195\n",
      "reconstruction loss is 0.2829236686229706\n",
      "reconstruction loss is 0.345967561006546\n",
      "reconstruction loss is 0.39889609813690186\n",
      "reconstruction loss is 0.33510538935661316\n",
      "reconstruction loss is 0.39367929100990295\n",
      "reconstruction loss is 0.2526373267173767\n",
      "reconstruction loss is 0.3058839738368988\n",
      "reconstruction loss is 0.2711386978626251\n",
      "reconstruction loss is 0.24753670394420624\n",
      "reconstruction loss is 0.39378273487091064\n",
      "reconstruction loss is 0.1858310103416443\n",
      "reconstruction loss is 0.36164620518684387\n",
      "reconstruction loss is 0.40244367718696594\n",
      "reconstruction loss is 0.4768204391002655\n",
      "reconstruction loss is 0.26847872138023376\n",
      "reconstruction loss is 0.36204609274864197\n",
      "reconstruction loss is 0.23680990934371948\n",
      "reconstruction loss is 0.4027352035045624\n",
      "reconstruction loss is 0.2652997672557831\n",
      "reconstruction loss is 0.2435530573129654\n",
      "reconstruction loss is 0.34698522090911865\n",
      "reconstruction loss is 0.2658032476902008\n",
      "reconstruction loss is 0.4291466474533081\n",
      "reconstruction loss is 0.3693908452987671\n",
      "reconstruction loss is 0.3299311101436615\n",
      "reconstruction loss is 0.3610290288925171\n",
      "reconstruction loss is 0.39384350180625916\n",
      "reconstruction loss is 0.31248223781585693\n",
      "reconstruction loss is 0.46619799733161926\n",
      "reconstruction loss is 0.3060578405857086\n",
      "reconstruction loss is 0.2329702526330948\n",
      "reconstruction loss is 0.26275476813316345\n",
      "reconstruction loss is 0.2858632504940033\n",
      "reconstruction loss is 0.2850617468357086\n",
      "reconstruction loss is 0.43810462951660156\n",
      "reconstruction loss is 0.3173823058605194\n",
      "reconstruction loss is 0.27941226959228516\n",
      "reconstruction loss is 0.3634258210659027\n",
      "reconstruction loss is 0.25241509079933167\n",
      "reconstruction loss is 0.2412676215171814\n",
      "reconstruction loss is 0.1990354210138321\n",
      "reconstruction loss is 0.3052770793437958\n",
      "reconstruction loss is 0.37355509400367737\n",
      "reconstruction loss is 0.23001785576343536\n",
      "reconstruction loss is 0.3838769197463989\n",
      "reconstruction loss is 0.2427365630865097\n",
      "reconstruction loss is 0.2120739370584488\n",
      "reconstruction loss is 0.3519822359085083\n",
      "reconstruction loss is 0.24922294914722443\n",
      "reconstruction loss is 0.35308316349983215\n",
      "reconstruction loss is 0.29581567645072937\n",
      "reconstruction loss is 0.35640859603881836\n",
      "reconstruction loss is 0.47245872020721436\n",
      "reconstruction loss is 0.30892011523246765\n",
      "reconstruction loss is 0.30141857266426086\n",
      "reconstruction loss is 0.26016852259635925\n",
      "reconstruction loss is 0.15637041628360748\n",
      "reconstruction loss is 0.4615533649921417\n",
      "reconstruction loss is 0.38920101523399353\n",
      "reconstruction loss is 0.3040761649608612\n",
      "reconstruction loss is 0.29462701082229614\n",
      "reconstruction loss is 0.26383402943611145\n",
      "reconstruction loss is 0.2865090072154999\n",
      "reconstruction loss is 0.29549577832221985\n",
      "reconstruction loss is 0.39980828762054443\n",
      "reconstruction loss is 0.24534940719604492\n",
      "reconstruction loss is 0.3465273678302765\n",
      "reconstruction loss is 0.30576106905937195\n",
      "reconstruction loss is 0.24964679777622223\n",
      "reconstruction loss is 0.24690799415111542\n",
      "reconstruction loss is 0.3214593827724457\n",
      "reconstruction loss is 0.32749029994010925\n",
      "reconstruction loss is 0.35622748732566833\n",
      "reconstruction loss is 0.2752092182636261\n",
      "reconstruction loss is 0.2375425100326538\n",
      "reconstruction loss is 0.3356464207172394\n",
      "reconstruction loss is 0.25962740182876587\n",
      "reconstruction loss is 0.3495759069919586\n",
      "reconstruction loss is 0.36376240849494934\n",
      "reconstruction loss is 0.3680518865585327\n",
      "reconstruction loss is 0.3836202919483185\n",
      "reconstruction loss is 0.38747385144233704\n",
      "reconstruction loss is 0.31730422377586365\n",
      "reconstruction loss is 0.34713301062583923\n",
      "reconstruction loss is 0.3988455832004547\n",
      "reconstruction loss is 0.3268015682697296\n",
      "reconstruction loss is 0.18134981393814087\n",
      "reconstruction loss is 0.3138519823551178\n",
      "reconstruction loss is 0.4190717041492462\n",
      "reconstruction loss is 0.3567667305469513\n",
      "reconstruction loss is 0.3334639072418213\n",
      "reconstruction loss is 0.16398979723453522\n",
      "reconstruction loss is 0.18653635680675507\n",
      "reconstruction loss is 0.38051724433898926\n",
      "reconstruction loss is 0.4388306140899658\n",
      "reconstruction loss is 0.3824321925640106\n",
      "reconstruction loss is 0.38942837715148926\n",
      "reconstruction loss is 0.30316004157066345\n",
      "reconstruction loss is 0.16569606959819794\n",
      "reconstruction loss is 0.28644922375679016\n",
      "reconstruction loss is 0.2492424100637436\n",
      "reconstruction loss is 0.29131391644477844\n",
      "reconstruction loss is 0.1988592892885208\n",
      "reconstruction loss is 0.3524649143218994\n",
      "reconstruction loss is 0.27047955989837646\n",
      "reconstruction loss is 0.32957717776298523\n",
      "reconstruction loss is 0.12750980257987976\n",
      "reconstruction loss is 0.2862999737262726\n",
      "reconstruction loss is 0.3496733009815216\n",
      "reconstruction loss is 0.21759526431560516\n",
      "reconstruction loss is 0.2629386782646179\n",
      "reconstruction loss is 0.24882690608501434\n",
      "reconstruction loss is 0.2442248910665512\n",
      "reconstruction loss is 0.25895991921424866\n",
      "reconstruction loss is 0.23010773956775665\n",
      "reconstruction loss is 0.3814760744571686\n",
      "reconstruction loss is 0.2895957827568054\n",
      "reconstruction loss is 0.3966865837574005\n",
      "reconstruction loss is 0.2529315948486328\n",
      "reconstruction loss is 0.3064779043197632\n",
      "reconstruction loss is 0.22465986013412476\n",
      "reconstruction loss is 0.3864037096500397\n",
      "reconstruction loss is 0.3455028831958771\n",
      "reconstruction loss is 0.16185618937015533\n",
      "reconstruction loss is 0.25194719433784485\n",
      "reconstruction loss is 0.34022021293640137\n",
      "reconstruction loss is 0.3070596754550934\n",
      "reconstruction loss is 0.206712007522583\n",
      "reconstruction loss is 0.17121829092502594\n",
      "reconstruction loss is 0.2592805325984955\n",
      "reconstruction loss is 0.3511523902416229\n",
      "reconstruction loss is 0.34132739901542664\n",
      "reconstruction loss is 0.35432854294776917\n",
      "----------------------------------------------------------------------\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "reconstruction loss is 0.3421001732349396\n",
      "reconstruction loss is 0.37112462520599365\n",
      "reconstruction loss is 0.29829227924346924\n",
      "reconstruction loss is 0.20031888782978058\n",
      "reconstruction loss is 0.2406102418899536\n",
      "reconstruction loss is 0.23959539830684662\n",
      "reconstruction loss is 0.22150832414627075\n",
      "reconstruction loss is 0.19903089106082916\n",
      "reconstruction loss is 0.21696513891220093\n",
      "reconstruction loss is 0.4428013861179352\n",
      "reconstruction loss is 0.24906247854232788\n",
      "reconstruction loss is 0.3654501736164093\n",
      "reconstruction loss is 0.29379287362098694\n",
      "reconstruction loss is 0.26047781109809875\n",
      "reconstruction loss is 0.3104887902736664\n",
      "reconstruction loss is 0.23469339311122894\n",
      "reconstruction loss is 0.34199008345603943\n",
      "reconstruction loss is 0.2068774253129959\n",
      "reconstruction loss is 0.3776134252548218\n",
      "reconstruction loss is 0.23322038352489471\n",
      "reconstruction loss is 0.2797083556652069\n",
      "reconstruction loss is 0.28269195556640625\n",
      "reconstruction loss is 0.4171089828014374\n",
      "reconstruction loss is 0.24069495499134064\n",
      "reconstruction loss is 0.1771600991487503\n",
      "reconstruction loss is 0.14666321873664856\n",
      "reconstruction loss is 0.2116691917181015\n",
      "reconstruction loss is 0.37678956985473633\n",
      "reconstruction loss is 0.23946698009967804\n",
      "reconstruction loss is 0.25899815559387207\n",
      "reconstruction loss is 0.2777482271194458\n",
      "reconstruction loss is 0.34416961669921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction loss is 0.33138468861579895\n",
      "reconstruction loss is 0.30341413617134094\n",
      "reconstruction loss is 0.2546052038669586\n",
      "reconstruction loss is 0.322433739900589\n",
      "reconstruction loss is 0.247453972697258\n",
      "reconstruction loss is 0.27619823813438416\n",
      "reconstruction loss is 0.1969505101442337\n",
      "reconstruction loss is 0.28265705704689026\n",
      "reconstruction loss is 0.267774760723114\n",
      "reconstruction loss is 0.4369959533214569\n",
      "reconstruction loss is 0.20129044353961945\n",
      "reconstruction loss is 0.25621920824050903\n",
      "reconstruction loss is 0.39109864830970764\n",
      "reconstruction loss is 0.22406315803527832\n",
      "reconstruction loss is 0.2054278701543808\n",
      "reconstruction loss is 0.19537241756916046\n",
      "reconstruction loss is 0.1543249487876892\n",
      "reconstruction loss is 0.26602646708488464\n",
      "reconstruction loss is 0.39629828929901123\n",
      "reconstruction loss is 0.38575685024261475\n",
      "reconstruction loss is 0.2602022886276245\n",
      "reconstruction loss is 0.23195497691631317\n",
      "reconstruction loss is 0.2820631265640259\n",
      "reconstruction loss is 0.294865220785141\n",
      "reconstruction loss is 0.3071236312389374\n",
      "reconstruction loss is 0.2873455584049225\n",
      "reconstruction loss is 0.3530513346195221\n",
      "reconstruction loss is 0.27115073800086975\n",
      "reconstruction loss is 0.20185910165309906\n",
      "reconstruction loss is 0.19209837913513184\n",
      "reconstruction loss is 0.20088689029216766\n",
      "reconstruction loss is 0.3383037745952606\n",
      "reconstruction loss is 0.2508848309516907\n",
      "reconstruction loss is 0.2374037355184555\n",
      "reconstruction loss is 0.20797671377658844\n",
      "reconstruction loss is 0.22345446050167084\n",
      "reconstruction loss is 0.2800966203212738\n",
      "reconstruction loss is 0.37284061312675476\n",
      "reconstruction loss is 0.34453871846199036\n",
      "reconstruction loss is 0.25527241826057434\n",
      "reconstruction loss is 0.35110047459602356\n",
      "reconstruction loss is 0.23672997951507568\n",
      "reconstruction loss is 0.3142752945423126\n",
      "reconstruction loss is 0.3235350251197815\n",
      "reconstruction loss is 0.2642379403114319\n",
      "reconstruction loss is 0.32554784417152405\n",
      "reconstruction loss is 0.2943432033061981\n",
      "reconstruction loss is 0.3402818739414215\n",
      "reconstruction loss is 0.21156974136829376\n",
      "reconstruction loss is 0.20546697080135345\n",
      "reconstruction loss is 0.2255154252052307\n",
      "reconstruction loss is 0.31907641887664795\n",
      "reconstruction loss is 0.21751753985881805\n",
      "reconstruction loss is 0.15046240389347076\n",
      "reconstruction loss is 0.2759268283843994\n",
      "reconstruction loss is 0.1903141289949417\n",
      "reconstruction loss is 0.2931927740573883\n",
      "reconstruction loss is 0.3430810868740082\n",
      "reconstruction loss is 0.1823672503232956\n",
      "reconstruction loss is 0.24929220974445343\n",
      "reconstruction loss is 0.1596696823835373\n",
      "reconstruction loss is 0.29546919465065\n",
      "reconstruction loss is 0.23975713551044464\n",
      "reconstruction loss is 0.19590796530246735\n",
      "reconstruction loss is 0.31054607033729553\n",
      "reconstruction loss is 0.2516733705997467\n",
      "reconstruction loss is 0.3523035943508148\n",
      "reconstruction loss is 0.3065430819988251\n",
      "reconstruction loss is 0.2129102498292923\n",
      "reconstruction loss is 0.19871795177459717\n",
      "reconstruction loss is 0.19799686968326569\n",
      "reconstruction loss is 0.21139518916606903\n",
      "reconstruction loss is 0.29206857085227966\n",
      "reconstruction loss is 0.19933174550533295\n",
      "reconstruction loss is 0.2139715552330017\n",
      "reconstruction loss is 0.33712664246559143\n",
      "reconstruction loss is 0.29240280389785767\n",
      "reconstruction loss is 0.2900463342666626\n",
      "reconstruction loss is 0.3734329044818878\n",
      "reconstruction loss is 0.2946263253688812\n",
      "reconstruction loss is 0.20815549790859222\n",
      "reconstruction loss is 0.29211142659187317\n",
      "reconstruction loss is 0.13446909189224243\n",
      "reconstruction loss is 0.20845139026641846\n",
      "reconstruction loss is 0.19050395488739014\n",
      "reconstruction loss is 0.14494048058986664\n",
      "reconstruction loss is 0.23163022100925446\n",
      "reconstruction loss is 0.29273346066474915\n",
      "reconstruction loss is 0.2432488203048706\n",
      "reconstruction loss is 0.05480597913265228\n",
      "reconstruction loss is 0.25491729378700256\n",
      "reconstruction loss is 0.23678700625896454\n",
      "reconstruction loss is 0.23974858224391937\n",
      "reconstruction loss is 0.16852021217346191\n",
      "reconstruction loss is 0.2806060016155243\n",
      "reconstruction loss is 0.24122782051563263\n",
      "reconstruction loss is 0.26597970724105835\n",
      "reconstruction loss is 0.3157714903354645\n",
      "reconstruction loss is 0.18448466062545776\n",
      "reconstruction loss is 0.23669762909412384\n",
      "reconstruction loss is 0.30280008912086487\n",
      "reconstruction loss is 0.31343135237693787\n",
      "reconstruction loss is 0.16265486180782318\n",
      "reconstruction loss is 0.24200914800167084\n",
      "reconstruction loss is 0.3567734658718109\n",
      "reconstruction loss is 0.30691662430763245\n",
      "reconstruction loss is 0.275534063577652\n",
      "reconstruction loss is 0.39253270626068115\n",
      "reconstruction loss is 0.15393123030662537\n",
      "reconstruction loss is 0.2922777533531189\n",
      "reconstruction loss is 0.23407985270023346\n",
      "reconstruction loss is 0.21536056697368622\n",
      "reconstruction loss is 0.11298879981040955\n",
      "reconstruction loss is 0.2533055245876312\n",
      "reconstruction loss is 0.13274426758289337\n",
      "reconstruction loss is 0.23450708389282227\n",
      "reconstruction loss is 0.26094239950180054\n",
      "reconstruction loss is 0.1579214632511139\n",
      "reconstruction loss is 0.375657320022583\n",
      "reconstruction loss is 0.17937666177749634\n",
      "reconstruction loss is 0.2763288915157318\n",
      "reconstruction loss is 0.3272809088230133\n",
      "reconstruction loss is 0.20268847048282623\n",
      "reconstruction loss is 0.22077326476573944\n",
      "reconstruction loss is 0.2266359180212021\n",
      "reconstruction loss is 0.2502632737159729\n",
      "reconstruction loss is 0.17749464511871338\n",
      "reconstruction loss is 0.2601429522037506\n",
      "reconstruction loss is 0.18401382863521576\n",
      "reconstruction loss is 0.2601798176765442\n",
      "reconstruction loss is 0.27148470282554626\n",
      "reconstruction loss is 0.21501685678958893\n",
      "reconstruction loss is 0.2056797742843628\n",
      "reconstruction loss is 0.17123496532440186\n",
      "reconstruction loss is 0.1424867957830429\n",
      "reconstruction loss is 0.1832389086484909\n",
      "reconstruction loss is 0.15888994932174683\n",
      "reconstruction loss is 0.16960608959197998\n",
      "reconstruction loss is 0.19958525896072388\n",
      "reconstruction loss is 0.43231236934661865\n",
      "reconstruction loss is 0.16139185428619385\n",
      "reconstruction loss is 0.3822312355041504\n",
      "reconstruction loss is 0.2990197539329529\n",
      "reconstruction loss is 0.20157290995121002\n",
      "reconstruction loss is 0.268388956785202\n",
      "reconstruction loss is 0.24719087779521942\n",
      "reconstruction loss is 0.27925819158554077\n",
      "reconstruction loss is 0.20512302219867706\n",
      "reconstruction loss is 0.2632816433906555\n",
      "reconstruction loss is 0.21252453327178955\n",
      "reconstruction loss is 0.32211390137672424\n",
      "reconstruction loss is 0.2336178570985794\n",
      "reconstruction loss is 0.2713218629360199\n",
      "reconstruction loss is 0.22509324550628662\n",
      "reconstruction loss is 0.14172033965587616\n",
      "reconstruction loss is 0.19717924296855927\n",
      "reconstruction loss is 0.2410275936126709\n",
      "reconstruction loss is 0.28138676285743713\n",
      "reconstruction loss is 0.17191167175769806\n",
      "reconstruction loss is 0.2796165347099304\n",
      "reconstruction loss is 0.18865014612674713\n",
      "reconstruction loss is 0.32436978816986084\n",
      "reconstruction loss is 0.28760620951652527\n",
      "reconstruction loss is 0.16439054906368256\n",
      "reconstruction loss is 0.23942697048187256\n",
      "reconstruction loss is 0.403214693069458\n",
      "reconstruction loss is 0.1913086324930191\n",
      "reconstruction loss is 0.18168850243091583\n",
      "reconstruction loss is 0.229048490524292\n",
      "reconstruction loss is 0.21413715183734894\n",
      "reconstruction loss is 0.16490814089775085\n",
      "reconstruction loss is 0.30374875664711\n",
      "reconstruction loss is 0.12629902362823486\n",
      "reconstruction loss is 0.253048300743103\n",
      "reconstruction loss is 0.17381928861141205\n",
      "reconstruction loss is 0.3336839973926544\n",
      "reconstruction loss is 0.16134978830814362\n",
      "reconstruction loss is 0.17900140583515167\n",
      "reconstruction loss is 0.20240844786167145\n",
      "reconstruction loss is 0.2053101807832718\n",
      "reconstruction loss is 0.1705912947654724\n",
      "reconstruction loss is 0.39404550194740295\n",
      "reconstruction loss is 0.2959669530391693\n",
      "reconstruction loss is 0.30541351437568665\n",
      "reconstruction loss is 0.25054457783699036\n",
      "reconstruction loss is 0.18574069440364838\n",
      "reconstruction loss is 0.2906358242034912\n",
      "reconstruction loss is 0.1906086653470993\n",
      "reconstruction loss is 0.23869669437408447\n",
      "reconstruction loss is 0.16783709824085236\n",
      "reconstruction loss is 0.222293421626091\n",
      "reconstruction loss is 0.22825567424297333\n",
      "reconstruction loss is 0.12380398064851761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction loss is 0.24095018208026886\n",
      "reconstruction loss is 0.35185983777046204\n",
      "reconstruction loss is 0.28852325677871704\n",
      "reconstruction loss is 0.1687774807214737\n",
      "reconstruction loss is 0.2814968526363373\n",
      "reconstruction loss is 0.32052114605903625\n",
      "reconstruction loss is 0.26338866353034973\n",
      "reconstruction loss is 0.21669451892375946\n",
      "reconstruction loss is 0.2318258136510849\n",
      "reconstruction loss is 0.2583489716053009\n",
      "reconstruction loss is 0.2391882687807083\n",
      "reconstruction loss is 0.20633800327777863\n",
      "reconstruction loss is 0.21645642817020416\n",
      "reconstruction loss is 0.16757990419864655\n",
      "reconstruction loss is 0.24565304815769196\n",
      "reconstruction loss is 0.2910510003566742\n",
      "reconstruction loss is 0.25112754106521606\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train(evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WdFTtc5yk55j",
    "outputId": "d89e2701-ef3e-4945-b9bc-ed405a3eb6cf"
   },
   "outputs": [],
   "source": [
    "#for _ in range(20):\n",
    "#    style_model.train_batch(input_idx, labels, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(style_model.state_dict(), 'style_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "big_camembert     embedding.pt           \u001b[0m\u001b[01;34mnotebooks\u001b[0m/    style_model\r\n",
      "classifier.ipynb  \u001b[01;34membeddings\u001b[0m/            \u001b[01;34m__pycache__\u001b[0m/  \u001b[01;34mtest_trainer\u001b[0m/\r\n",
      "dataset.csv       itos.pickle            README.md     Untitled.ipynb\r\n",
      "dataset.csv.1     minimal_example.ipynb  \u001b[01;34msrcs\u001b[0m/\r\n",
      "\u001b[01;34mdatasets\u001b[0m/         model_synonyms.ipynb   stoi.pickle\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "minimal_example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
