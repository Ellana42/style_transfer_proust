{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b966a662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da0a8e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "syns=[synset.lemma_names('fra') for synset in wordnet.synsets('bien', lang='fra')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86325c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ellana/.local/lib/python3.9/site-packages/transformers/pipelines/token_classification.py:135: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'U',\n",
       "  'score': 0.99467033,\n",
       "  'word': 'Face',\n",
       "  'start': 0,\n",
       "  'end': 4},\n",
       " {'entity_group': 'P', 'score': 0.99961555, 'word': 'à', 'start': 4, 'end': 6},\n",
       " {'entity_group': 'DET',\n",
       "  'score': 0.9995907,\n",
       "  'word': 'un',\n",
       "  'start': 6,\n",
       "  'end': 9},\n",
       " {'entity_group': 'NC',\n",
       "  'score': 0.99955326,\n",
       "  'word': 'choc',\n",
       "  'start': 9,\n",
       "  'end': 14},\n",
       " {'entity_group': 'ADJ',\n",
       "  'score': 0.9991835,\n",
       "  'word': 'inédit',\n",
       "  'start': 14,\n",
       "  'end': 21},\n",
       " {'entity_group': 'P',\n",
       "  'score': 0.37106535,\n",
       "  'word': ',',\n",
       "  'start': 21,\n",
       "  'end': 22},\n",
       " {'entity_group': 'DET',\n",
       "  'score': 0.99959034,\n",
       "  'word': 'les',\n",
       "  'start': 22,\n",
       "  'end': 26},\n",
       " {'entity_group': 'NC',\n",
       "  'score': 0.99956495,\n",
       "  'word': 'mesures',\n",
       "  'start': 26,\n",
       "  'end': 34},\n",
       " {'entity_group': 'VPP',\n",
       "  'score': 0.99886703,\n",
       "  'word': 'mises',\n",
       "  'start': 34,\n",
       "  'end': 40},\n",
       " {'entity_group': 'P',\n",
       "  'score': 0.9996246,\n",
       "  'word': 'en',\n",
       "  'start': 40,\n",
       "  'end': 43},\n",
       " {'entity_group': 'NC',\n",
       "  'score': 0.99953294,\n",
       "  'word': 'place',\n",
       "  'start': 43,\n",
       "  'end': 49},\n",
       " {'entity_group': 'P',\n",
       "  'score': 0.9996234,\n",
       "  'word': 'par',\n",
       "  'start': 49,\n",
       "  'end': 53},\n",
       " {'entity_group': 'DET',\n",
       "  'score': 0.99959344,\n",
       "  'word': 'le',\n",
       "  'start': 53,\n",
       "  'end': 56},\n",
       " {'entity_group': 'NC',\n",
       "  'score': 0.9995371,\n",
       "  'word': 'gouvernement',\n",
       "  'start': 56,\n",
       "  'end': 69},\n",
       " {'entity_group': 'V',\n",
       "  'score': 0.9993771,\n",
       "  'word': 'ont',\n",
       "  'start': 69,\n",
       "  'end': 73},\n",
       " {'entity_group': 'VPP',\n",
       "  'score': 0.99911004,\n",
       "  'word': 'permis',\n",
       "  'start': 73,\n",
       "  'end': 80},\n",
       " {'entity_group': 'DET',\n",
       "  'score': 0.99958867,\n",
       "  'word': 'une',\n",
       "  'start': 80,\n",
       "  'end': 84},\n",
       " {'entity_group': 'NC',\n",
       "  'score': 0.9995635,\n",
       "  'word': 'protection',\n",
       "  'start': 84,\n",
       "  'end': 95},\n",
       " {'entity_group': 'ADJ',\n",
       "  'score': 0.9991781,\n",
       "  'word': 'forte',\n",
       "  'start': 95,\n",
       "  'end': 101},\n",
       " {'entity_group': 'CC',\n",
       "  'score': 0.9991297,\n",
       "  'word': 'et',\n",
       "  'start': 101,\n",
       "  'end': 104},\n",
       " {'entity_group': 'ADJ',\n",
       "  'score': 0.9992274,\n",
       "  'word': 'efficace',\n",
       "  'start': 104,\n",
       "  'end': 113},\n",
       " {'entity_group': 'P+D',\n",
       "  'score': 0.99933004,\n",
       "  'word': 'des',\n",
       "  'start': 113,\n",
       "  'end': 117},\n",
       " {'entity_group': 'NC',\n",
       "  'score': 0.9995596,\n",
       "  'word': 'ménages',\n",
       "  'start': 117,\n",
       "  'end': 125}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gilf/french-camembert-postag-model\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"gilf/french-camembert-postag-model\")\n",
    "\n",
    "nlp_token_class = pipeline('ner', model=model, tokenizer=tokenizer, grouped_entities=True)\n",
    "nlp_token_class('Face à un choc inédit, les mesures mises en place par le gouvernement ont permis une protection forte et efficace des ménages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d1948d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/sentence_dataset.csv', sep='|', index_col=0).dropna().sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb17f6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    text = re.sub(\"~@~Y\", \"'\", text)\n",
    "    text = re.sub( \"(@\\w*\\\\b\\s?|#\\w*\\\\b\\s?|&\\w*\\\\b\\s?|\\n\\s?|\\\\\\\\|\\<|\\>|\\||\\*)\", \"\", text)\n",
    "    text = re.sub(\"\\/\", \"\", text)\n",
    "    text = re.sub(\"l'\", \"le \", text)\n",
    "    text = re.sub(\"d'\", \"de \", text)\n",
    "    text = re.sub(\"j'\", \"je \", text)\n",
    "    text = re.sub(\"qu'\", \"que \", text)\n",
    "    text = re.sub(\"t'\", \"te \", text)\n",
    "    text = re.sub(\"c'\", \"ce \", text)\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = re.compile('<.*?>').sub('', text)\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub( ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub(r'\\[[0-9]*\\]', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2f94ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned'] = df.text.apply(cleaning)\n",
    "df['tokenized'] = df.cleaned.apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6240a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proust = df[df.label == 1]\n",
    "df_news = df[df.label == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4c0509f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_freq_table(dataframe):\n",
    "    out = pd.DataFrame.from_dict(Counter(dataframe.tokenized.explode().to_list()), orient='index')\n",
    "    out = out.rename(columns={'index':'token', 0:\"freq\"}).sort_values('freq', ascending=False)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f2c5de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "proust = to_freq_table(df_proust)\n",
    "news = to_freq_table(df_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3f8b7fc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (4003056971.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [25]\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def get_pos(word):\n",
    "    pass\n",
    "\n",
    "def get_right_syns(word):\n",
    "    pass\n",
    "\n",
    "def get_most_common_corpus_syn(word):\n",
    "    pass\n",
    "\n",
    "def transfer_style_sentence(sentence):\n",
    "    decomposition = nlp_token_class(sentence)\n",
    "    for word in decomposition:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc99dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO lemmatize then delemmatize ?\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "875a73e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corres_pos(camembert_pos):\n",
    "    if camembert_pos == 'NC':\n",
    "        return wn.NOUN\n",
    "    if camembert_pos[0] == 'V':\n",
    "        return wn.VERB\n",
    "    if camembert_pos[:3] == 'ADJ':\n",
    "        return wn.ADJ\n",
    "    if camembert_pos[:3] == 'ADV':\n",
    "        return wn.ADV\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dacc9e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>▁livres</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>▁he</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>do</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>▁oct</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>icité</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>▁vigueur</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>▁secteur</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>▁bien</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>▁précis</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>302 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          freq\n",
       "▁livres      1\n",
       "▁he          1\n",
       "b            1\n",
       "do           1\n",
       "▁oct         1\n",
       "...        ...\n",
       "icité        1\n",
       "▁vigueur     1\n",
       "▁secteur     1\n",
       "▁bien        1\n",
       "▁précis      1\n",
       "\n",
       "[302 rows x 1 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "db630516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " livres hebdo october des pseudonymes et du second degré les contributeurs dont le crayon porte dordinaire plutôt à gauche ont tous retourné leurs vestes pour loccasion et se sont mis dans la peau de fervents supporters de nicolas sarkozy à quelques semaines des primaires\n",
      "livres hebdo october des pseudonymes et du second degré les contributeurs dont le crayon porte dordinaire plutôt à gauche ont tous retourné leurs vestes pour loccasion et se sont mis dans la peau de fervents supporters de nico las sarkozy à quelques semaines des primaires\n"
     ]
    }
   ],
   "source": [
    "sentence = df.iloc[0].cleaned\n",
    "tagged = nlp_token_class(sentence)\n",
    "new_sentence = []\n",
    "for word in tagged:\n",
    "    pos = get_corres_pos(word['entity_group'])\n",
    "    if pos is None:\n",
    "        new_sentence.append(word['word'])\n",
    "    else:\n",
    "        syns = [synset.lemma_names('fra') for synset in wordnet.synsets(word['word'], lang='fra', pos=pos)]\n",
    "        if len(syns) == 0:\n",
    "            new_sentence.append(word['word'])\n",
    "        else:\n",
    "            syns = list(set([word for syn in syns for word in syn]))\n",
    "            syns = {''.join(tokenizer.tokenize(word)): word for word in syns}\n",
    "            good_tokens = news.index.intersection([syn for syn in syns.keys()])\n",
    "            if len(good_tokens) == 0:\n",
    "                new_sentence.append(word['word'])\n",
    "            else:\n",
    "                most_common_syn = news.loc[good_tokens].iloc[0].name\n",
    "                new_sentence.append(syns[most_common_syn])\n",
    "print(sentence)\n",
    "print(' '.join(new_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf896f",
   "metadata": {},
   "outputs": [],
   "source": [
    ".index.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71d2a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet.synsets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60adf5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.VERB)\n",
    "[Synset('chase.v.01')]\n",
    "\n",
    "The other parts of speech are NOUN, ADJ and ADV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
